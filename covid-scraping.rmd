---
title: 'COVID Scraping'
author: 'Jeffrey Brennan'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(readxl)
library(writexl)
library(dplyr)
library(stringr)
library(jsonlite)
library(rjson)
```

```{r helper files & functions}
# Grab every sheet from an excel file and convert to list of dataframes
# https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames
read_excel_allsheets = function(filename, tibble = FALSE) {
    sheets = readxl::excel_sheets(filename)
    x = lapply(sheets, function(X) read_excel(filename, sheet = X, skip = 1,
                                              col_names = TRUE, na = '.'))
    x = lapply(x, as.data.frame)
    return(x)
}

# set data for writing files
# If before 5 PM, then record as last date since DSHS data will not be updated yet
# TODO: see if this can be removed in place of using the max date on a consistent dataframe
date_out = ifelse((Sys.time() < as.POSIXct(paste0(Sys.Date(), '16:00'), tz = 'America/Chicago')),
                   Sys.Date() - 1,
                   Sys.Date())

# convert numeric sys.date() to yyyy-mm-dd
date_out = as.Date(date_out, origin = '1970-1-1')
```

# CENSUS 

## County

```{r}
# Source: https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-detail.html
county_demo = read.csv('original-sources/census/county_demo.csv')

# restrict to 2018 pop estimate & drop extra cols
county_demo_2018 = subset(county_demo[, c(5:ncol(county_demo))], YEAR == 11)

# Data cleaning (drop county suffix & total age group); rename county
county_demo_2018$YEAR = NULL
county_demo_2018$CTYNAME = gsub(' County', '', county_demo_2018$CTYNAME)
county_demo_2018 = county_demo_2018[which(county_demo_2018$AGEGRP != 0), ]
colnames(county_demo_2018)[1] = 'County'

# add age labels
county_demo_2018$AGEGRP = factor(county_demo_2018$AGEGRP,
                                 labels = c('0-4', '5-9', '10-14', '15-19', '20-24', '25-29',
                                            '30-34', '35-39', '40-44', '45-49', '50-54', '55-59',
                                            '60-64',  '65-69', '70-74', '75-79', '80-84', '85+'))
```


## City

```{r}
# keep only 2019 population estimate
city_pops = read.csv('original-sources/census/city_pops.csv')[, c(3, 4, 9, 22)]
colnames(city_pops) = c('County_Code', 'Place_Code', 'City', 'Population')

# drop city name suffixes 
city_pops$City = gsub(' city| town| village', '', city_pops$City)

# drop pt suffix
# pt indicates overlap of cities between counties
city_pops$City = gsub(' (pt.)', '', city_pops$City, fixed = TRUE)

# get county FIPS code for verification
county_fips = subset(city_pops, Place_Code == 0)[-1, c(1,3)]
colnames(county_fips) = c('County_Code', 'County')

# add FIPS to counties
merged_city = merge(city_pops, county_fips, by = 'County_Code')

# drop county totals from list (based on Place_Code)
merged_city = subset(merged_city, Place_Code != 0 & Place_Code != 99990)[, 2:4]
colnames(merged_city)[1] = 'FIPS'
```

# COUNTY LEVEL

## Google mobility

```{r}
# fread for faster processing
mobility_data = fread('https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv')
```

```{r}
# filter mobility
mobility_texas = subset(mobility_data, sub_region_1 == 'Texas')

# drop cols
mobility_texas = mobility_texas[, -c(1:3, 5:7)]
write.csv(mobility_texas, 'original-sources/GOOGLE.csv', row.names = F)

# fix colnames
colnames(mobility_texas) = c('County', 'Date', 'Retail_Recreation', 'Grocery_Pharmacy',
                             'Parks', 'Transit', 'Workplaces', 'Residential')

# Add name for blank county cells & drop 'county' suffix
mobility_texas$County = sub('^$', 'Unallocated', mobility_texas$County)
mobility_texas$County = gsub(' County', '', mobility_texas$County)

# drop blank cells
mobility_texas = subset(mobility_texas, County != 'Unallocated')

#fix types
mobility_texas$County = as.factor(mobility_texas$County)
mobility_texas$Date = as.Date(mobility_texas$Date)
```

## DSHS 

### cases

```{r}
# download xlsx as tempfile and load using readxl
case_url = 'http://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyCaseCountData.xlsx'
temp = tempfile()
download.file(case_url, temp, mode = 'wb') 
DSHS_cases_time = data.frame(read_excel(temp, sheet = 1))

# fix colnames & select correct rows
colnames(DSHS_cases_time) = DSHS_cases_time[2, ]
DSHS_cases_time = DSHS_cases_time[3:(nrow(DSHS_cases_time) - 11), ]
colnames(DSHS_cases_time)[1] = 'County'

#save
write.csv(DSHS_cases_time, 'original-sources/DSHS_county_cases.csv', row.names = FALSE)

# convert wide data to long
DSHS_cases_long = reshape::melt(DSHS_cases_time, id = c('County'))
colnames(DSHS_cases_long) = c('County', 'Date', 'Cases_Cumulative')

# fix dates (remove linebreaks and return characters from column, then interpret as %m-%d)
DSHS_cases_long$Date = as.Date(gsub('Cases|\r|\r|\n', '', DSHS_cases_long$Date), format = '%m-%d')

# force as integer -> coerce -- to NA
# if a date is missing, fill using previous cumulative cases
# calculate daily cases by padding the initial cumulative value, then reporting the difference between successive values by county
DSHS_cases_long$Cases_Cumulative = as.integer(as.character(DSHS_cases_long$Cases_Cumulative))

# DSHS_cases_long_NA = 
#   DSHS_cases_long %>% 
#     group_by(County) %>% 
#     tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
#     mutate(Cases_Daily = c(Cases_Cumulative[1], diff(Cases_Cumulative)))

DSHS_cases_long = 
  DSHS_cases_long %>% 
    group_by(County) %>%
    tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
    mutate(Cases_Cumulative_NA = Cases_Cumulative) %>% 
    tidyr::fill(Cases_Cumulative, .direction = "down") %>%
    mutate(Cases_Daily = c(Cases_Cumulative[1], diff(Cases_Cumulative)))
```

### deaths

```{r}
death_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyFatalityCountData.xlsx'
temp = tempfile()
download.file(death_url, temp, mode = 'wb') 
DSHS_deaths_time = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colnames
colnames(DSHS_deaths_time) = DSHS_deaths_time[2, ]
last_row = which(DSHS_deaths_time[, 1] == 'Zavala')

DSHS_deaths_time = DSHS_deaths_time[3:last_row, ]
colnames(DSHS_deaths_time)[1] = 'County'

colnames(DSHS_deaths_time) = trimws(gsub('Fatalities', '', colnames(DSHS_deaths_time)))

# fix dates (DSHS included 2 different formats at random intervals)
# numeric format -> convert to date using excel numbering convetion
# non-numeric format -> coerce m/d/y to yyy-mm-dd
dates = colnames(DSHS_deaths_time)[2:length(colnames(DSHS_deaths_time))]
dates = as.Date(dates, format = '%m-%d')
# numeric_dates = which(is.na(as.Date(dates, format = '%m-%d')))
# dates[numeric_dates] = format(as.Date(as.integer(dates[numeric_dates]), origin = '1899-12-30'), '%Y-%m-%d')
# dates[-numeric_dates] = format(as.Date(dates[-numeric_dates], format = '%m-%d'), '%Y-%m-%d')

# replace date values
colnames(DSHS_deaths_time)[2:length(colnames(DSHS_deaths_time))] = format(dates, '%Y-%m-%d')

#save
write.csv(DSHS_deaths_time, 'original-sources/DSHS_county_deaths.csv', row.names = FALSE)

# melt
DSHS_deaths_long = reshape::melt(DSHS_deaths_time, id = c('County'))
colnames(DSHS_deaths_long) = c('County', 'Date', 'Deaths_Cumulative')

# fix most county names (ex: HARRIS -> Harris)
DSHS_deaths_long$County = stringr::str_to_title(DSHS_deaths_long$County)

# manually fix exceptions using DSHS_cases_long as dictionary
mismatched_counties = setdiff(unique(DSHS_deaths_long$County), unique(DSHS_cases_long$County))
DSHS_deaths_long$County = gsub('De Witt', 'DeWitt', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mcculloch', 'McCulloch', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mclennan', 'McLennan', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mcmullen', 'McMullen', DSHS_deaths_long$County)

# check again - 0 indicates all fixed
length(setdiff(unique(DSHS_deaths_long$County), unique(DSHS_cases_long$County)))

# calculate daily deaths & fill cumulative deaths
DSHS_deaths_long$Deaths_Cumulative = as.integer(as.character(DSHS_deaths_long$Deaths_Cumulative))

# DSHS_deaths_long_NA = 
#   DSHS_deaths_long %>%
#   mutate(Date = as.Date(Date)) %>%
#   group_by(County) %>%
#   tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
#   mutate(Deaths_Daily = c(Deaths_Cumulative[1], diff(Deaths_Cumulative)))

DSHS_deaths_long = 
  DSHS_deaths_long %>%
  mutate(Date = as.Date(Date)) %>%
  group_by(County) %>%
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Deaths_Cumulative_NA = Deaths_Cumulative) %>%
  tidyr::fill(Deaths_Cumulative, .direction = "down") %>%
  mutate(Deaths_Daily = c(Deaths_Cumulative[1], diff(Deaths_Cumulative)))
```

### testing

```{r}
test_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19CumulativeTestsOverTimebyCounty.xlsx'
temp = tempfile()
download.file(test_url, temp, mode = 'wb') 
DSHS_tests_time = data.frame(readxl::read_excel(temp, sheet = 1))

# TEMP FIX drop first duplicated date column
dupe_dates = which(DSHS_tests_time[1, ] == 'Tests Through June 23')
DSHS_tests_time = DSHS_tests_time[, -dupe_dates[1]]

# fix colnames
colnames(DSHS_tests_time) = DSHS_tests_time[1, ]
DSHS_tests_time = DSHS_tests_time[2:(nrow(DSHS_tests_time) - 9), ]
colnames(DSHS_tests_time)[1] = 'County'

#save
write.csv(DSHS_tests_time, 'original-sources/DSHS_county_tests.csv', row.names = FALSE)

# melt
DSHS_tests_long = reshape::melt(DSHS_tests_time, id = c('County'))
colnames(DSHS_tests_long) = c('County', 'Date', 'Tests_Cumulative')

# fix dates
DSHS_tests_long$Date = gsub('*', '', DSHS_tests_long$Date, fixed = T)
DSHS_tests_long$Date = as.Date(gsub('Tests Through ', '', DSHS_tests_long$Date), format = '%B%d')

# replace '--' and '-' with NA
DSHS_tests_long$Tests_Cumulative = na_if(DSHS_tests_long$Tests_Cumulative, '-')
DSHS_tests_long$Tests_Cumulative = na_if(DSHS_tests_long$Tests_Cumulative, '--')

DSHS_tests_long$Tests_Cumulative = as.numeric(as.character(DSHS_tests_long$Tests_Cumulative))

# calculate daily tests
# DSHS_tests_long_NA = 
#     DSHS_tests_long %>% 
#     group_by(County) %>% 
#     tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
#     mutate(Tests_Daily = c(Tests_Cumulative[1], diff(Tests_Cumulative)))

DSHS_tests_long = 
  DSHS_tests_long %>% 
  group_by(County) %>% 
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Tests_Cumulative_NA = Tests_Cumulative) %>%
  tidyr::fill(Tests_Cumulative, .direction = "down") %>%
  mutate(Tests_Daily = c(Tests_Cumulative[1], diff(Tests_Cumulative)))

# drop unmergable counties 
# DSHS_tests_long_NA = subset(DSHS_tests_long_NA, County != 'Unknown' & County != 'Pending Assignments')
DSHS_tests_long = subset(DSHS_tests_long, County != 'Unknown' & County != 'Pending Assignments')
```

### active cases

```{r}
active_case_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19ActiveCaseDatabyCounty.xlsx'
temp = tempfile()
download.file(active_case_url, temp, mode = 'wb') 
DSHS_active_cases_time = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colname
colnames(DSHS_active_cases_time) = DSHS_active_cases_time[2, ]
DSHS_active_cases_time = DSHS_active_cases_time[3:(nrow(DSHS_active_cases_time)), 2:ncol(DSHS_active_cases_time)]
colnames(DSHS_active_cases_time)[1] = 'County'

#save
write.csv(DSHS_active_cases_time, 'original-sources/DSHS_county_active_cases.csv', row.names = FALSE)

# melt
DSHS_active_cases_long = reshape::melt(DSHS_active_cases_time, id = c('County'))
colnames(DSHS_active_cases_long) = c('County', 'Date', 'Active_Cases_Cumulative')

# fix dates
DSHS_active_cases_long$Date = as.Date(gsub('Active|Cases|\r|\n', '',
                                           DSHS_active_cases_long$Date),
                                      format = '%m-%d')

# calculate daily active_cases & drop duplicate dates (07-24 DSHS duplicated)
# DSHS_active_cases_long_NA =  
#     DSHS_active_cases_long %>% 
#     group_by(County) %>% 
#     unique() %>%
#     mutate(Active_Cases_Cumulative = as.integer(as.character(Active_Cases_Cumulative))) %>%
#     tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
#     mutate(Active_Cases_Daily = c(Active_Cases_Cumulative[1], diff(Active_Cases_Cumulative)))

DSHS_active_cases_long =  
    DSHS_active_cases_long %>% 
    group_by(County) %>% 
    unique() %>%
    mutate(Active_Cases_Cumulative = as.integer(as.character(Active_Cases_Cumulative))) %>%
    tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
    mutate(Active_Cases_Cumulative_NA = Active_Cases_Cumulative) %>%
    tidyr::fill(Active_Cases_Cumulative, .direction = "down") %>%
    mutate(Active_Cases_Daily = c(Active_Cases_Cumulative[1], diff(Active_Cases_Cumulative)))
```

```{r}
# combine DSHS sources using merge helper function
# https://www.musgraveanalytics.com/blog/2018/2/12/how-to-merge-multiple-data-frames-using-base-r

DSHS_county_counts = Reduce(function(x, y) merge(x, y, by = c('Date', 'County'), all=TRUE),
       list(DSHS_cases_long, DSHS_deaths_long, DSHS_tests_long, DSHS_active_cases_long))
# 
# # drop population - will be using census data
# DSHS_county_counts$Population.x = NULL
# DSHS_county_counts$Population.y = NULL
```

## Classifications

```{r}
# add metro and PHR code designations
# source: https://www.dshs.state.tx.us/chs/info/TxCoPhrMsa.xls]
county_classifications = read_xlsx('original-sources/county_classifications.xlsx', sheet = 1)[1:254, c(1, 5, 8)]

# set colnames
colnames(county_classifications) = c('County', 'PHR', 'Metro_Area')

# add PHR readable names from https://dshs.texas.gov/regions/default.shtm
PHR_helper = data.frame(PHR = unique(county_classifications$PHR))
PHR_helper$PHR_Name = c('Tyler PHR', 'El Paso PHR', 'Harlingen PHR', 
                        'Arlington PHR', 'Lubbock PHR', 'San Antonio PHR',
                        'Houston PHR', 'Temple PHR')

county_classifications = merge(county_classifications, PHR_helper, by = 'PHR')
```


```{r}
# TSA levels
tsa_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19HospitalizationsOverTimebyTSA.xlsx'
download.file(tsa_url, 'original-sources/DSHS_tsa_hosp.xlsx', mode = 'wb') 

DSHS_tsa_names = readxl::read_xlsx('original-sources/DSHS_tsa_hosp.xlsx')[3:24, 1:2]
colnames(DSHS_tsa_names) = c('TSA', 'TSA_Name')

# drop . suffix from TSA codes
DSHS_tsa_names$TSA = gsub('.', '', DSHS_tsa_names$TSA, fixed = TRUE)

# get list of counties per TSA
tsa = read.csv('original-sources/tsa_list.csv', header = F)[-1]
tsa_long = reshape::melt(tsa, id = c('V2', 'V3'))
tsa_long_complete = subset(tsa_long, value != '')[, c(1, 4)]
colnames(tsa_long_complete) = c('TSA', 'County')

tsa_long_complete$County = trimws(tsa_long_complete$County)
tsa_long_complete = merge(tsa_long_complete, DSHS_tsa_names, by = 'TSA') %>% unique()
```

## merge

```{r}
# DSHS pop - removed by DSHS 07/30 - defaulting to previous file
dshs_pops = unique(read.csv('https://raw.githubusercontent.com/jeffbrennan/COVID-19/d03d476f7fb060dfd2e1a600a6a1e449df0ab8df/original-sources/DSHS_county_cases.csv')[, c('County', 'Population')])
colnames(dshs_pops) = c('County', 'Population_DSHS')

merged_dshs = Reduce(function(x, y) merge(x, y, by = 'County', all = TRUE),
                       list(DSHS_county_counts, tsa_long_complete, dshs_pops, county_classifications))

# add TSA & PHR combination
merged_dshs$TSA_Combined = paste0(merged_dshs$TSA, ' - ', merged_dshs$TSA_Name)
merged_dshs$PHR_Combined = paste0(merged_dshs$PHR, ' - ', merged_dshs$PHR_Name)

merged_county = as.data.frame(merge(merged_dshs, mobility_texas,
                                    by = c('Date', 'County'), all = TRUE)) %>% 
  filter(!is.na(County) & County != 'Unknown')

# fix types
merged_county$County = as.factor(merged_county$County)
merged_county$Population_DSHS = as.numeric(merged_county$Population_DSHS)

# keep only relevant dates (previous dates include google mobility only)
merged_county = merged_county %>% 
  filter(Date >= as.Date('2020-03-04') & !is.na(County)) %>% 
  unique()
```

# TSA LEVEL

## Computed

```{r}
# longitudinal counts (sum)
DSHS_tsa_counts =
    merged_county %>%
    group_by(Date, TSA, TSA_Name) %>% 
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

# static pop counts (sum)
DSHS_tsa_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(TSA) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

# longitudinal google data (mean)
DSHS_tsa_google = 
  merged_county %>%
  group_by(Date, TSA, TSA_Name) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

DSHS_tsa = merge(DSHS_tsa_counts, DSHS_tsa_google, by = c('Date', 'TSA', 'TSA_Name'))
DSHS_tsa = merge(DSHS_tsa, DSHS_tsa_pops, by = 'TSA', all = TRUE)
```

## DSHS hospitals

```{r}
hosp_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19HospitalizationsOverTimebyTSA.xlsx'
download.file(hosp_url, 'original-sources/DSHS_tsa_hosp.xlsx', mode = 'wb') 
DSHS_tsa_hosp = read_excel_allsheets('original-sources/DSHS_tsa_hosp.xlsx')

DSHS_hosp_clean = function(df, var_name) { 
  colnames(df) = df[1, ]
  df = df[2:23, c(1, 3:ncol(df))]
  df$`TSA ID` = gsub('.', '', df$`TSA ID`, fixed = TRUE)

  # add temp fix for DSHS duplicated dates (8/8) (fixed by DSHS 8/10)
  # TODO: add dynamic handling
  if (length(grep('.x', colnames(df)) > 0)) { 
    df = df[, -grep('.x', colnames(df))]
    colnames(df) = gsub('.y', '', colnames(df))
  }

  # TODO: make date conversion function
  dates = colnames(df)[2:length(colnames(df))]
  numeric_dates = which(is.na(as.Date(dates, format = '%Y-%m-%d')))
  
  # convert from 5 digit excel numeric format
  # https://stackoverflow.com/questions/43230470/how-to-convert-excel-date-format-to-proper-date-in-r
  dates[numeric_dates] = format(as.Date(as.integer(dates[numeric_dates]), origin = '1899-12-30'))
  dates[-numeric_dates] = format(as.Date(dates[-numeric_dates]), '%Y-%m-%d')
  colnames(df)[2:length(colnames(df))] = dates

  df_long = reshape::melt(df, id = 'TSA ID')
  colnames(df_long) = c('TSA', 'Date', var_name)
  df_long$Date = as.Date(df_long$Date)
  return(df_long)
}

# 1,2,3 references the 3 sheets produced by DSHS
hosp_1 = DSHS_hosp_clean(DSHS_tsa_hosp[[1]], 'Hospitalizations_Total')
hosp_2 = DSHS_hosp_clean(DSHS_tsa_hosp[[2]], 'Hospitalizations_General')
hosp_3 = DSHS_hosp_clean(DSHS_tsa_hosp[[3]], 'Hospitalizations_ICU')


if(length(which(hosp_1$Date == '2008-08-08')) > 0) {
  hosp_1$Date[which(hosp_1$Date == '2008-08-08')] = as.Date('2020-08-08')
}

if(length(which(hosp_2$Date == '2008-08-08')) > 0) {
  hosp_2$Date[which(hosp_2$Date == '2008-08-08')] = as.Date('2020-08-08')
}
```


## hospital capacity

```{r}
hosp_cap_url = 'https://dshs.texas.gov/coronavirus/TexasHospitalCapacityoverTimebyTSA.xlsx'
download.file(hosp_cap_url, 'original-sources/DSHS_tsa_hosp_cap.xlsx', mode = 'wb') 
DSHS_tsa_hosp_cap = read_excel_allsheets('original-sources/DSHS_tsa_hosp_cap.xlsx')

hosp_cap1 = DSHS_hosp_clean(DSHS_tsa_hosp_cap[[1]], 'Beds_Available_Total')
hosp_cap2 = DSHS_hosp_clean(DSHS_tsa_hosp_cap[[2]], 'Beds_Available_ICU')
hosp_cap3 = DSHS_hosp_clean(DSHS_tsa_hosp_cap[[3]], 'Beds_Occupied_Total')
hosp_cap4 = DSHS_hosp_clean(DSHS_tsa_hosp_cap[[4]], 'Beds_Occupied_ICU')

hosp_cap4$Beds_Occupied_ICU = gsub('--', NA, hosp_cap4$Beds_Occupied_ICU)

# DSHS duplicated Aug 7th available ICU Beds - fixed 8/17
# if(length(which(hosp_cap2$Date == '2020-08-07')) > 22) { 
#   hosp_cap2$Date[which(hosp_cap2$Date == '2020-08-07')[23:44]] = as.Date('2020-08-08')
# }
```



## DSHS dashboard

```{r}
# pull DSHS dashboard data using link found from inspect element -> network
DSHS_json_hosp_tsa = jsonlite::fromJSON("https://services5.arcgis.com/ACaLB9ifngzawspq/arcgis/rest/services/DSHS_COVID_Hospital_Data/FeatureServer/0/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outSR=102100&resultOffset=0&resultRecordCount=25&resultType=standard&cacheHint=true")[['features']][['attributes']]

DSHS_json_hosp_tsa = DSHS_json_hosp_tsa[, c(2,5:9)]

colnames(DSHS_json_hosp_tsa) = c('TSA', 'Hospital_Beds_Staffed', 'Hospital_Beds_Available',
                                 'ICU_Beds_Available', 'Ventilators_Available', 'Current_Cases')
DSHS_json_hosp_tsa$Date = format(date_out, '%m/%d/%y')

# add computed cols
DSHS_json_hosp_tsa = DSHS_json_hosp_tsa %>%
                    mutate(Hospital_Beds_Taken = Hospital_Beds_Staffed - Hospital_Beds_Available - Current_Cases)

# export todays file
write.csv(DSHS_json_hosp_tsa, paste0('original-sources/historical/hosp/tsa_hosp_', date_out, '.csv'),
          row.names = FALSE)

# read in all files
hosp_list <- paste0('original-sources/historical/hosp/',
                    list.files(path = 'original-sources/historical/hosp',
                               pattern = '*.csv'))

tsa_all_hosp = lapply(hosp_list, read.csv, fileEncoding = 'UTF-8-BOM')
tsa_combined_hosp = rbindlist(tsa_all_hosp, fill = TRUE)

tsa_combined_hosp$Date = as.Date(tsa_combined_hosp$Date, format = '%m/%d/%y')
# save
write.csv(tsa_combined_hosp, 'original-sources/DSHS_tsa_hosp_detail.csv', row.names = FALSE)


# only keep vents - rest of data is now being posted by DSHS directly 
tsa_combined_hosp = tsa_combined_hosp[, c('TSA', 'Date', 'Ventilators_Available')]
```

## merge

```{r}
merged_tsa = Reduce(function(x, y) merge(x, y, by = c('Date', 'TSA'), all = TRUE),
                    list(DSHS_tsa, tsa_combined_hosp,
                         hosp_1, hosp_2, hosp_3,
                         hosp_cap1, hosp_cap2, hosp_cap3, hosp_cap4))

# fix types & ensure TSA values are valid
merged_tsa = merged_tsa %>% 
  mutate_at(vars(Beds_Available_Total, Ventilators_Available, Beds_Available_ICU,
                 Beds_Occupied_Total, Beds_Occupied_ICU, Hospitalizations_Total,
                 Hospitalizations_General, Hospitalizations_ICU),
            funs(as.character)) %>%
  mutate_at(vars(Beds_Available_Total, Ventilators_Available, Beds_Available_ICU,
                 Beds_Occupied_Total, Beds_Occupied_ICU, Hospitalizations_Total,
                 Hospitalizations_General, Hospitalizations_ICU),
            funs(as.integer)) %>%
  mutate(TSA_Combined = paste0(TSA, ' - ', TSA_Name)) %>%
  filter(!is.na(TSA)) %>% 
  unique()
```

# PHR LEVEL

## computed

```{r}
DSHS_phr_counts =
    merged_county %>%
    group_by(Date, PHR, PHR_Name) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

DSHS_phr_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(PHR) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

DSHS_phr_google = 
  merged_county %>%
  group_by(Date, PHR) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

DSHS_phr = merge(DSHS_phr_counts, DSHS_phr_google, by = c('Date', 'PHR'))
DSHS_phr = merge(DSHS_phr, DSHS_phr_pops, by = c('PHR'), all = TRUE)
```

## alf
(Assisted Living Facilities)

```{r}
download.file('https://dshs.texas.gov/coronavirus/COVID-19inALFsoverTimebyRegion.xlsx',
               destfile = 'original-sources/DSHS_alf.xlsx', mode = 'wb')

DSHS_alf = read_excel_allsheets('original-sources/DSHS_alf.xlsx')

Clean_Facility = function(df, count_type) { 
  df = df[1:8, ]
  colnames(df)[1] = 'PHR'
  df_long = reshape::melt(df, id = 'PHR')
  
  colnames(df_long) = c('PHR', 'Date', count_type)
  df_long$Date = as.Date(as.integer(df_long$Date), origin = '2020-05-14')
  df_long[, 3] = as.numeric(df_long[, 3])
  return(df_long)
  }

alf_fac_totals = Clean_Facility(DSHS_alf[[1]], 'ALF_Total')
alf_cases = Clean_Facility(DSHS_alf[[2]], 'ALF_Cases')
alf_deaths = Clean_Facility(DSHS_alf[[3]], 'ALF_Deaths')
alf_recoveries = Clean_Facility(DSHS_alf[[4]], 'ALF_Recoveries')

alf_df = Reduce(function(x, y) merge(x, y, by = c('Date', 'PHR'), all = TRUE),
                list(alf_fac_totals, alf_cases, alf_deaths, alf_recoveries))
```

## nursing

```{r}
download.file('https://dshs.texas.gov/coronavirus/COVID-19inNursingHomesoverTimebyRegion.xlsx',
               destfile = 'original-sources/DSHS_nursing.xlsx', mode = 'wb')

DSHS_nursing = read_excel_allsheets('original-sources/DSHS_nursing.xlsx')

nursing_fac_totals = Clean_Facility(DSHS_nursing[[1]], 'Nursing_Total')
nursing_cases = Clean_Facility(DSHS_nursing[[2]], 'Nursing_Cases')
nursing_deaths = Clean_Facility(DSHS_nursing[[3]], 'Nursing_Deaths')
nursing_recoveries = Clean_Facility(DSHS_nursing[[4]], 'Nursing_Recoveries')

nursing_df = Reduce(function(x, y) merge(x, y, by = c('Date', 'PHR'), all = TRUE),
                list(nursing_fac_totals, nursing_cases, nursing_deaths, nursing_recoveries))
```

## merge

```{r}
phr_df = Reduce(function(x, y) merge(x, y, by = c('Date', 'PHR'), all = TRUE),
                list(DSHS_phr, alf_df, nursing_df))

# address error when knitting despite str(phr_df indicating all calc columns are numeric)
phr_df$Nursing_Cases = as.numeric(phr_df$Nursing_Cases)
phr_df$Nursing_Recoveries = as.numeric(phr_df$Nursing_Recoveries)

phr_df = phr_df %>% filter(!is.na(PHR)) %>% unique()
```

# METRO LEVEL

## computed

```{r}
DSHS_metro_counts =
    merged_county %>%
    group_by(Date, Metro_Area) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

DSHS_metro_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(Metro_Area) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

DSHS_metro_google = 
  merged_county %>%
  group_by(Date, Metro_Area) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)
```


## merge

```{r}
DSHS_metro = subset(merge(DSHS_metro_counts, DSHS_metro_pops,
                          by = 'Metro_Area', all = TRUE), !is.na(Date))

DSHS_metro = merge(DSHS_metro, DSHS_metro_google, by = c('Date', 'Metro_Area'))
DSHS_metro = DSHS_metro %>% filter(!is.na(Metro_Area)) %>% unique()
```


# STATE LEVEL

## HHS facility reporting

```{r}
# read in files
hhs_fac_reporting_raw = read.csv(url('https://opendata.arcgis.com/datasets/adf5753521ed4e2199d6a4c246e08f84_0.csv'))

hhs_date = as.Date(hhs_fac_reporting_raw$last_updated)
hhs_fac_reporting_raw$Date = hhs_date
hhs_fac_reporting_raw$last_updated = NULL

# combine csv for archival
write_xlsx(list('1' = hhs_fac_reporting_raw),
           path = paste0('original-sources/historical/hhs/hhs_data_', hhs_date, '.xlsx'))

# read in archival sources
hhs_list <- paste0('original-sources/historical/hhs/',
                    list.files(path = 'original-sources/historical/hhs',
                               pattern = '*.xlsx'))

HHS_fac_clean = function(df) { 
    df$Date = format(as.Date(df$Date, origin = '1899-12-30'), '%Y-%m-%d')
    
    if (df$Date[1] <= as.Date('2020-08-03')) {
    
    clean_df = df %>% 
      filter(state_name == 'Texas') %>% 
      dplyr::select(-c('ï..OBJECTID', 'Shape__Area', 'Shape__Length', 'state_fips',
                'state_abbr', 'hhs_region', 'state_name'))
    
    colnames(clean_df)[1:3] = c('reporting_hospitals', 'total_hospitals', 'percent_reporting')
    colnames(clean_df)[1:3] = paste0('HHS_', colnames(clean_df)[1:3])
    
    # drop date from colnames (will likely change over time)
    colnames(clean_df) = gsub('_*\\d', '', colnames(clean_df))
    
  } else { 
    clean_df = df %>% 
      filter(state_name == 'Texas') %>% 
      dplyr::select(-c('ï..OBJECTID', 'state_name'))
    
    colnames(clean_df)[1:3] = paste0('HHS_', colnames(clean_df)[1:3])
    }
  return(clean_df)
}

# 1st lapply: read excel sheets
# 2nd: clean all of them
# rbind cleaned sheets together
HHS_fac_reporting <- rbindlist(lapply(lapply(hhs_list, read_xlsx, sheet = 1), HHS_fac_clean))
```

## HHS hospitalization

```{r}
HHS_hosp_clean = function(df, data_type) { 
  clean_df = df %>% 
    filter(state == 'TX') %>% 
    dplyr::select(-c('state'))
  
  # append type of data to colnames
  colnames(clean_df)[c(3:4, 6:7)] = paste0(data_type, colnames(clean_df[c(3:4, 6:7)]))
  colnames(clean_df) = paste0('HHS_', colnames(clean_df))
  
  clean_df$Date = as.Date(clean_df$HHS_collection_date)
  clean_df$HHS_collection_date = NULL

  return(clean_df)
}
# timeseries files 
HHS_all_inpatient = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/reported_inpatient_all_20200720_0537.csv')),
    'All.Inpatient.')

HHS_covid_inpatient = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/inpatient_covid_final_20200720_0537.csv')),
    'COVID.Inpatient.')

HHS_all_icu = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/icu_final_20200720_0537.csv')),
    'All.ICU.')
```


### merge

```{r}
# merge together for combination with other state data
hhs_df = Reduce(function(x, y) merge(x, y, by = 'Date', all = TRUE),
                list(HHS_all_inpatient, HHS_covid_inpatient, HHS_all_icu, HHS_fac_reporting))

```


## DSHS (time series)

```{r}
state_url = 'https://www.dshs.state.tx.us/coronavirus/TexasCOVID19CaseCountData.xlsx'

download.file(state_url, destfile = 'original-sources/DSHS_state.xlsx', mode = 'wb')
dshs_header = colnames(read_excel('original-sources/DSHS_state.xlsx', sheet = 9))[1]

current_year =  substr(Sys.Date(), 1, 4)
dshs_date = paste0(current_year, '/',  str_extract_all(dshs_header, '\\d*\\/\\d*')[[1]])
dshs_date = format(as.Date(dshs_date), '%Y_%m_%d')

# save state level file to historical database for longitudinal demo
download.file(state_url, paste0('original-sources/historical/demo/dshs_',
                                dshs_date, '.xlsx'), mode = 'wb')

DSHS_state = read_excel_allsheets('original-sources/DSHS_state.xlsx')
```


```{r}
DSHS_tests = DSHS_state[[4]]
DSHS_tests = DSHS_tests[1:(nrow(DSHS_tests) - 3), ]

colnames(DSHS_tests) = c('Date', 'Viral_Tests', 'Antibody_Tests', 'Antigen Tests',  'Tests_Total', 'Positive_Rate', 'Viral_Tests_New_Avg', 'Antibody_Tests_New_Avg', 'Total_Tests_New_Avg')

# fix date (https://stackoverflow.com/questions/43230470/how-to-convert-excel-date-format-to-proper-date-in-r) 
DSHS_tests$Date = as.Date(as.integer(DSHS_tests$Date), origin = '1899-12-30')

# fix calc col type
DSHS_tests$Positive_Rate = as.numeric(DSHS_tests$Positive_Rate)
```


```{r}
# drop row num
DSHS_hospitalizations = DSHS_state[[9]][, c(2:3)]

# drop footnote
DSHS_hospitalizations = DSHS_hospitalizations[1:(nrow(DSHS_hospitalizations) - 2), ]

# fix date
DSHS_hospitalizations$Date = as.Date(DSHS_hospitalizations$Date, format = '%Y%m%d')

# set colnames
colnames(DSHS_hospitalizations)[2] = 'Hospital_Total'
```


```{r}
# merge tests & hospitalizations
DSHS_state_time = merge(DSHS_tests, DSHS_hospitalizations, by = 'Date', all = TRUE)
```

## DSHS (day counts)

```{r}
# avoids duplication of results between sheets
DSHS_state_day = data.frame(Recovered_Total = DSHS_state[[3]][1,1],
                            Active_Total = DSHS_state[[3]][1,2],
                            Tests_State_Lab = DSHS_state[[5]][1,2],
                            Tests_Commercial_Lab = DSHS_state[[5]][2,2],
                            Antigen_Tests_Total = DSHS_state[[6]][1,2],
                            Antigen_Tests_Positive = DSHS_state[[6]][2,2],
                            Antibody_Tests_Total = DSHS_state[[7]][1,2],
                            Antibody_tests_Positive = DSHS_state[[7]][2,2],
                            Hospital_Bed_Total = DSHS_state[[8]][2,2],
                            Hospital_Bed_Available = DSHS_state[[8]][3,2],
                            ICU_Bed_Available = DSHS_state[[8]][4,2],
                            Ventilator_Available = DSHS_state[[8]][5,2],
                            Case_Investigations = DSHS_state[[10]][[14,2]],
                            Death_Investigations = DSHS_state[[13]][14,2])
```

## DSHS Demographics

```{r}
# read in all files
demo.list <- paste0('original-sources/historical/demo/',
                    list.files(path = 'original-sources/historical/demo',
                               pattern = '*.xlsx'))

DSHS_case_age <- append(lapply(demo.list[1:66], read_excel, sheet = 9),
                        lapply(demo.list[67:length(demo.list)], read_excel, sheet = 10))
                        
DSHS_case_gender <- append(lapply(demo.list[1:66], read_excel, sheet = 10),
                           lapply(demo.list[67:length(demo.list)], read_excel, sheet = 11))

DSHS_case_race <- append(lapply(demo.list[1:66], read_excel, sheet = 11),
                        lapply(demo.list[67:length(demo.list)], read_excel, sheet = 12))

DSHS_death_age <- append(lapply(demo.list[1:66], read_excel, sheet = 12),
                         lapply(demo.list[67:length(demo.list)], read_excel, sheet = 13))

DSHS_death_gender <- append(lapply(demo.list[1:66], read_excel, sheet = 13),
                            lapply(demo.list[67:length(demo.list)], read_excel, sheet = 14))

DSHS_death_race <- append(lapply(demo.list[1:66], read_excel, sheet = 14),
                          lapply(demo.list[67:length(demo.list)], read_excel, sheet = 15))

DSHS_demo_clean = function(df) {
  # search for updated date (digit "/" digit)
  date = str_extract_all(colnames(df)[1], '\\d+\\/\\d+')[[1]]
  df = df[2:(nrow(df) - 4), ]
  
  colnames(df) = c('Group', 'var_Cumulative', 'var_PCT')

  # remove any rows containing total
  total_check = which(df[, 'Group'] == 'Total')
  if (length(total_check) != 0) {df = df[-which(df[, 'Group'] == 'Total'), ]}

  # ensure values are numeric, PCT reported in decimals, and drop coerced NAs (eg. Gender 7/8)
  df = df %>%
    mutate(Date = as.Date(paste0('2020/', date))) %>%
    mutate(var_PCT = gsub('%','',var_PCT)) %>%
    mutate_at(c('var_Cumulative', 'var_PCT'), as.numeric) %>% 
    mutate(var_PCT = ifelse(var_PCT > 1, var_PCT/100, var_PCT)) %>%
    na.omit()

  return(df)
}

# calculate daily cases/deaths based on cumulative counts
Demo_Daily = function(df, count_type) { 
  df = df %>% 
    group_by(Group) %>% 
    mutate(var_Daily = as.numeric(c(var_Cumulative[1], diff(var_Cumulative))))
    
  colnames(df) = gsub('var', count_type, colnames(df))
  
  return(df)
  }

# clean all files, row bind them together, then calculate daily counts
DSHS_case_age_df = Demo_Daily(rbindlist(lapply(DSHS_case_age, DSHS_demo_clean)), count_type = 'Cases')
DSHS_case_gender_df = Demo_Daily(rbindlist(lapply(DSHS_case_gender, DSHS_demo_clean)), count_type = 'Cases')
DSHS_case_race_df = Demo_Daily(rbindlist(lapply(DSHS_case_race, DSHS_demo_clean)), count_type = 'Cases')

DSHS_death_age_df = Demo_Daily(rbindlist(lapply(DSHS_death_age, DSHS_demo_clean)), count_type = 'Deaths')
DSHS_death_gender_df = Demo_Daily(rbindlist(lapply(DSHS_death_gender, DSHS_demo_clean)), count_type = 'Deaths')
DSHS_death_race_df = Demo_Daily(rbindlist(lapply(DSHS_death_race, DSHS_demo_clean)), count_type = 'Deaths')

# combine case and death cols
DSHS_age_df = merge(DSHS_case_age_df, DSHS_death_age_df, by = c('Date', 'Group'))
DSHS_gender_df = merge(DSHS_case_gender_df, DSHS_death_gender_df, by = c('Date', 'Group'))
DSHS_race_df = merge(DSHS_case_race_df, DSHS_death_race_df, by = c('Date', 'Group'))
```

## Computed

```{r}
state_counts =
    merged_county %>%
    group_by(Date) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum(., na.rm = TRUE)))

state_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(Date) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

state_google = 
  merged_county %>%
  group_by(Date) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

state_facilities = 
  phr_df %>%
  group_by(Date) %>%
  summarize_at(vars(ALF_Total, ALF_Cases, ALF_Deaths, ALF_Recoveries,
                    Nursing_Total, Nursing_Cases, Nursing_Deaths, Nursing_Recoveries),
               funs(sum))

state_hosp_detail = 
  merged_tsa %>% 
  group_by(Date) %>%
  summarize_at(vars(Hospitalizations_Total, Hospitalizations_General, Hospitalizations_ICU,
                    Beds_Available_Total, Beds_Available_ICU, Beds_Occupied_Total, Beds_Occupied_ICU,
                    Ventilators_Available),
               funs(sum))
```

## merge

```{r}
merged_state = Reduce(function(x, y) merge(x, y, by = c('Date'), all=TRUE),
       list(state_counts, state_google, DSHS_state_time, state_facilities, state_hosp_detail, hhs_df))

merged_state$Population_DSHS = state_pops$Population_DSHS

merged_state = merged_state %>% 
  filter(Date >= as.Date('2020-03-04')) %>% 
  unique()
```

# OUTPUT 

## City

```{r}
# write.csv(merged_city, 'tableau/city_pops.csv', row.names = F)
``` 

## County

```{r}
merged_county_out = merged_county %>% 
  dplyr::select(-c(Cases_Cumulative_NA, Deaths_Cumulative_NA,
                   Tests_Cumulative_NA, Active_Cases_Cumulative_NA))

write.csv(merged_county_out, file = 'combined-datasets/county.csv', row.names = F)
write.csv(merged_county_out, 'tableau/county.csv', row.names = FALSE)
write.csv(county_demo_2018, file = 'combined-datasets/county_demo.csv', row.names = F)
```

<!-- ### NA cumulative cases when missing -->

<!-- ```{r} -->
<!-- merged_county_NA = merged_county %>% -->
<!--   group_by(County) %>% -->
<!--   mutate(Cases_Cumulative = Cases_Cumulative_NA) %>% -->
<!--   mutate(Deaths_Cumulative = Deaths_Cumulative_NA) %>% -->
<!--   mutate(Tests_Cumulative = Tests_Cumulative_NA) %>% -->
<!--   mutate(Active_Cases_Cumulative = Active_Cases_Cumulative_NA) %>%  -->
<!--   mutate(Cases_Daily = c(Cases_Cumulative[1], diff(Cases_Cumulative))) %>% -->
<!--   mutate(Deaths_Daily = c(Deaths_Cumulative[1], diff(Deaths_Cumulative))) %>% -->
<!--   mutate(Tests_Daily = c(Tests_Cumulative[1], diff(Tests_Cumulative))) %>% -->
<!--   mutate(Active_Cases_Daily = c(Active_Cases_Cumulative[1], diff(Active_Cases_Cumulative))) %>%  -->
<!--   dplyr::select(-c(Cases_Cumulative_NA, Deaths_Cumulative_NA, -->
<!--                    Tests_Cumulative_NA, Active_Cases_Cumulative_NA)) -->

<!-- write.csv(merged_county_NA, 'tableau/county_cumulative_NA.csv', row.names = F) -->
<!-- ``` -->



## TSA

```{r}
write.csv(merged_tsa, file = 'combined-datasets/tsa.csv', row.names = F)

hosp_tsa = merged_tsa[, c(1:3, 18:26)] %>% filter(Date >= min(hosp_cap1$Date))
write.csv(hosp_tsa, file = 'tableau/hospitalizations_tsa.csv', row.names = F)
```

## PHR 

```{r}
write.csv(phr_df, file = 'combined-datasets/phr.csv', row.names = F)
# write.csv(phr_df, file = 'tableau/phr.csv', row.names = F)
```

## Metro

```{r}
write.csv(DSHS_metro, file = 'combined-datasets/metro.csv', row.names = F)
```

## State

```{r}
state_out = list("longitudinal" = merged_state, "current" = DSHS_state_day)
write_xlsx(state_out, path = "combined-datasets/state.xlsx")
```

## Demographics

```{r}
demo_out = list("age" = DSHS_age_df, "gender" = DSHS_gender_df, "race" = DSHS_race_df)
write_xlsx(demo_out, path = "combined-datasets/demographics.xlsx")
# write_xlsx(demo_out, path = "tableau/demographics.xlsx")
```

### stacked

```{r}
DSHS_age_df$Group_Type = 'Age'
DSHS_race_df$Group_Type = 'Race'
DSHS_gender_df$Group_Type = 'Gender'

demo_stacked = rbind(DSHS_age_df, DSHS_race_df, DSHS_gender_df)
write.csv(demo_stacked, file = 'tableau/stacked_demographics.csv', row.names = FALSE)
```
