---
title: 'COVID Scraping'
author: 'Jeffrey Brennan'
output: html_document
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "diagnostics/") })
---

# SETUP

```{r, echo = FALSE}
# performance analysis 
# source: https://bookdown.org/yihui/rmarkdown-cookbook/time-chunk.html
all_times <- list()  # store the time for each chunk
knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) {
      now <<- Sys.time()
    } else {
      res <- difftime(Sys.time(), now)
      all_times[[options$label]] <<- res
    }
  }
}))
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(time_it = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(comment = NA)
```

```{r, echo = FALSE}
refactor_version = '2'
```

```{r}
# data manipulation
library(data.table)
library(readxl)
library(writexl)
library(dplyr)
library(stringr)
library(zoo)

# plotting
library(ggplot2)
library(ggpubr)

# web scraping
library(rvest)
library(jsonlite)
```

```{r}
# Grab every sheet from an excel file and convert to list of dataframes
# https://stackoverflow.com/questions/12945687/read-all-worksheets-in-an-excel-workbook-into-an-r-list-with-data-frames
read_excel_allsheets = function(filename, tibble = FALSE) {
    sheets = readxl::excel_sheets(filename)
    x = lapply(sheets, function(X) read_excel(filename, sheet = X, skip = 1,
                                              col_names = TRUE, na = '.'))
    x = lapply(x, as.data.frame)
    return(x)
}

# set date for writing files
# If before 5 PM, then record as last date since DSHS data will not be updated yet
# TODO: see if this can be removed in place of using the max date on a consistent dataframe
date_out = ifelse((Sys.time() < as.POSIXct(paste0(Sys.Date(), '16:00'), tz = 'America/Chicago')),
                   Sys.Date() - 1,
                   Sys.Date())

# convert numeric sys.date() to yyyy-mm-dd
date_out = as.Date(date_out, origin = '1970-1-1')
```

# CENSUS

## County

```{r}
# Source: https://www.census.gov/data/tables/time-series/demo/popest/2010s-counties-detail.html
county_demo = read.csv('original-sources/census/county_demo.csv')

# restrict to 2018 pop estimate & drop extra cols
county_demo_2018 = subset(county_demo[, c(5:ncol(county_demo))], YEAR == 11)

# Data cleaning (drop county suffix & total age group); rename county
county_demo_2018$YEAR = NULL
county_demo_2018$CTYNAME = gsub(' County', '', county_demo_2018$CTYNAME)
county_demo_2018 = county_demo_2018[which(county_demo_2018$AGEGRP != 0), ]
colnames(county_demo_2018)[1] = 'County'

# add age labels
county_demo_2018$AGEGRP = factor(county_demo_2018$AGEGRP,
                                 labels = c('0-4', '5-9', '10-14', '15-19', '20-24', '25-29',
                                            '30-34', '35-39', '40-44', '45-49', '50-54', '55-59',
                                            '60-64',  '65-69', '70-74', '75-79', '80-84', '85+'))
```

## City

```{r}
# keep only 2019 population estimate
city_pops = read.csv('original-sources/census/city_pops.csv')[, c(3, 4, 9, 22)]
colnames(city_pops) = c('County_Code', 'Place_Code', 'City', 'City_Population')

# drop city name suffixes 
city_pops$City = gsub(' city| town| village', '', city_pops$City)

# drop pt suffix
# pt indicates overlap of cities between counties
city_pops$City = gsub(' (pt.)', '', city_pops$City, fixed = TRUE)

# get county FIPS code for verification
county_fips = subset(city_pops, Place_Code == 0)[-1, c(1,3)]
colnames(county_fips) = c('County_Code', 'County')

# add FIPS to counties
merged_city = merge(city_pops, county_fips, by = 'County_Code')

# drop county totals from list (based on Place_Code)
merged_city = subset(merged_city, Place_Code != 0 & Place_Code != 99990)[, 2:4]
colnames(merged_city)[1] = 'FIPS'
```

## State

```{r}
# source: https://www.census.gov/quickfacts/fact/table/TX/HSG860218#HSG860218
# state_demo = 
#   data.frame(Race = c('Black', 'Other', 'Asian', 'Other', 'Other', 'Hispanic', 'White'),
#              PCT = c(.129, .01, .052, .001, .021, .397, .412)) %>% 
#   group_by(Race) %>% 
#   summarize(PCT = sum(PCT))
# 
# write.csv(state_demo, 'tableau/state_demographics.csv', row.names = FALSE)
```


# SCHOOL LEVEL

```{r}
# school files are small (~ 6kb - read every time and don't overwrite)
nyt_schools = rbindlist(lapply(list.files('original-sources/historical/schools/', full.names = TRUE), read.csv)) %>% 
  setNames(c('School', 'City', 'County', 'Deaths_Cumulative', 'Cases_Cumulative', 'Date')) %>%
  mutate(Date = as.Date(Date)) %>% 
  mutate(Cases_Cumulative = ifelse(Cases_Cumulative == -1, NA, Cases_Cumulative))
```

## Texas school districts

```{r}
# data ending on Sunday posted on Thursday (4 day lag)
school_dates = seq(as.Date('2020-09-20'), by = 'week', length.out = 52)
school_date = max(school_dates[which(school_dates <= date_out - 4)])


tryCatch({ 
  school_url = paste0('https://dshs.texas.gov/chs/data/tea/district-level-school-covid-19-case-data/District-level-data-file-',
                    format(school_date+2, '%m%d%Y'), '.xls')
  temp = tempfile()
  download.file(school_url, temp, mode = 'wb')
  DSHS_schools_raw = data.frame(read_excel(temp, sheet = 1))
  },
  # if error, retry url using _ instead of - in filename
  error = function(e) { 
   school_url = paste0('https://dshs.texas.gov/chs/data/tea/district-level-school-covid-19-case-data/District-level-data-file_',
                    format(school_date+2, '%m%d%Y'), '.xls')
  temp = tempfile()
  download.file(school_url, temp, mode = 'wb')
  DSHS_schools_raw <<- data.frame(read_excel(temp, sheet = 1))
  
  })


first_row = which(DSHS_schools_raw[, 1] == 'District Name') + 1
last_row = which(DSHS_schools_raw[, 1] == 'ZEPHYR ISD')
colnames(DSHS_schools_raw)[1:2] = c('District', 'LEA')

county_school_link = read.csv('original-sources/Directory.csv') %>%
  dplyr::select(County.Name, District.Number, District.Name, District.City, District.Zip) %>%
  unique() %>%
  mutate(District.Number = as.numeric(gsub("'", '', as.character(District.Number))))

district_metadata = read.csv('original-sources/Current_Districts.csv') %>%
  dplyr::select(NAME, SDLEA10, DISTRICT_N, NCES_DISTR) %>%
  filter(!is.na(SDLEA10)) %>%
  unique() %>%
  merge(., county_school_link, by.x = 'DISTRICT_N', by.y = 'District.Number', all = TRUE)

DSHS_schools = DSHS_schools_raw[first_row:last_row, ] %>%
  mutate(LEA = as.numeric(gsub("'", '', LEA))) %>%
  merge(., district_metadata, by.x = 'LEA', by.y = 'DISTRICT_N', all.x = TRUE) %>%
  dplyr::select(-NAME, -NCES_DISTR, -SDLEA10, -District.Name) %>%
  dplyr::select(District, LEA, County.Name, District.City, District.Zip, everything())
```

```{r}
# modify county and cities
DSHS_schools_clean = DSHS_schools %>%
  mutate(District = str_to_title(District)) %>% 
  mutate(District = gsub('Isd', 'ISD', District)) %>%
  mutate(County.Name = gsub(' COUNTY', '', County.Name)) %>%
  mutate(County.Name = stringr::str_to_title(County.Name)) %>%
  mutate(County.Name = gsub('Dewitt', 'DeWitt', County.Name)) %>%
  mutate(County.Name = gsub('Mcculloch', 'McCulloch', County.Name)) %>%
  mutate(County.Name = gsub('Mclennan', 'McLennan', County.Name)) %>%
  mutate(County.Name = gsub('Mcmullen', 'McMullen', County.Name)) %>%
  mutate(District.City = stringr::str_to_title(District.City)) %>%
  mutate(District.Zip = substr(as.character(District.Zip), 1, 5)) %>%
  mutate(Date = school_date) %>%
  dplyr::select(District, Date, LEA, County.Name,
                District.City, District.Zip, everything()) %>%
  mutate_at(7:ncol(.), as.numeric) %>%
  setNames(c('District', 'Date', 'LEA', 'County', 'City', 'ZIP',
             'Total_Enrollment', 'Approximate_Enrollment', 'Cases_Weekly_GRADE_EE_3', 'Cases_Weekly_GRADE_4_6', 'Cases_Weekly_GRADE_7_12',
             'Cases_Weekly_Staff', 'Infections_Weekly_On_Campus',
             'Infections_Weekly_Off_Campus', 'Infections_Weekly_Unknown',

             'Cases_Cumulative_GRADE_EE_3', 'Cases_Cumulative_GRADE_4_6',
             'Cases_Cumulative_GRADE_7_12', 'Cases_Cumulative_Staff',
             'Infections_Cumulative_On_Campus', 'Infections_Cumulative_Off_Campus',
             'Infections_Cumulative_Unknown'))
full_counties = read_excel('original-sources/county_classifications.xlsx', sheet=1)[, 1] %>% unlist()

# no schools reporting from loving, all other counties match
setdiff(unique(full_counties), unique(DSHS_schools_clean$County))

# save for future combinations - avoids cleaning on every run
write.csv(DSHS_schools_clean,
          paste0('original-sources/historical/dshs-schools/DSHS_Schools_', school_date, '.csv'),
          row.names = FALSE)
```

```{r}
# combine previous cleaned versions w/ current week
DSHS_school_df = rbindlist(lapply(list.files('original-sources/historical/dshs-schools', full.names = TRUE), read.csv))

write.csv(DSHS_school_df, 'tableau/district_school_reopening.csv', row.names = FALSE)
```

# COUNTY LEVEL

## Google mobility

```{r}
# fread for faster processing
# TODO: run once a week instead of daily
mobility_data = fread('https://www.gstatic.com/covid19/mobility/Global_Mobility_Report.csv')
```

```{r}
# filter mobility
mobility_texas = subset(mobility_data, sub_region_1 == 'Texas')

# drop cols
mobility_texas = mobility_texas[, -c(1:3, 5:7)]
write.csv(mobility_texas, 'original-sources/GOOGLE.csv', row.names = F)

# fix colnames
colnames(mobility_texas) = c('County', 'Date', 'Retail_Recreation', 'Grocery_Pharmacy',
                             'Parks', 'Transit', 'Workplaces', 'Residential')

# Add name for blank county cells & drop 'county' suffix
mobility_texas$County = sub('^$', 'Unallocated', mobility_texas$County)
mobility_texas$County = gsub(' County', '', mobility_texas$County)

# drop blank cells
mobility_texas = subset(mobility_texas, County != 'Unallocated')

#fix types
mobility_texas$County = as.factor(mobility_texas$County)
mobility_texas$Date = as.Date(mobility_texas$Date)
```

## cases

```{r}
# download xlsx as tempfile and load using readxl
case_url = 'http://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyCaseCountData.xlsx'
temp = tempfile()
download.file(case_url, temp, mode = 'wb') 
DSHS_cases_time = data.frame(read_excel(temp, sheet = 1))

# fix colnames & select correct rows
colnames(DSHS_cases_time) = DSHS_cases_time[2, ]
DSHS_cases_time = DSHS_cases_time[3:(nrow(DSHS_cases_time) - 11), ]
colnames(DSHS_cases_time)[1] = 'County'

#save
write.csv(DSHS_cases_time, 'original-sources/DSHS_county_cases.csv', row.names = FALSE)

# convert wide data to long
DSHS_cases_long = reshape::melt(DSHS_cases_time, id = c('County'))
colnames(DSHS_cases_long) = c('County', 'Date', 'Cases_Cumulative')

# fix dates (remove linebreaks and return characters from column, then interpret as %m-%d)
DSHS_cases_long$Date = as.Date(gsub('Cases|\r|\r|\n', '', DSHS_cases_long$Date), format = '%m-%d')

# force as integer -> coerce -- to NA
# if a date is missing, fill using previous cumulative cases
# calculate daily cases by padding the initial cumulative value, then reporting the difference between successive values by county
DSHS_cases_long$Cases_Cumulative = as.integer(as.character(DSHS_cases_long$Cases_Cumulative))

DSHS_cases_long = 
  DSHS_cases_long %>% 
    group_by(County) %>%
    tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
    mutate(Cases_Cumulative_NA = Cases_Cumulative) %>% 
    tidyr::fill(Cases_Cumulative, .direction = "down")
    # mutate(Cases_Daily = c(Cases_Cumulative[1], diff(Cases_Cumulative)))
```

## deaths

```{r}
death_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID19DailyCountyFatalityCountData.xlsx'
temp = tempfile()
download.file(death_url, temp, mode = 'wb') 
DSHS_deaths_time = data.frame(readxl::read_excel(temp, sheet = 1))

# fix colnames
colnames(DSHS_deaths_time) = DSHS_deaths_time[2, ]
last_row = which(DSHS_deaths_time[, 1] == 'ZAVALA')

DSHS_deaths_time = DSHS_deaths_time[3:last_row, ]
colnames(DSHS_deaths_time)[1] = 'County'

colnames(DSHS_deaths_time) = trimws(gsub('Fatalities', '', colnames(DSHS_deaths_time)))

# fix dates (DSHS included 2 different formats at random intervals)
# numeric format -> convert to date using excel numbering convetion
# non-numeric format -> coerce m/d/y to yyy-mm-dd
dates = colnames(DSHS_deaths_time)[2:length(colnames(DSHS_deaths_time))]
dates = format(as.Date(dates, format = '%m-%d'), '%Y-%m-%d')
# numeric_dates = which(is.na(as.Date(dates, format = '%m-%d')))
# dates[numeric_dates] = format(as.Date(as.integer(dates[numeric_dates]), origin = '1899-12-30'), '%Y-%m-%d')
# dates[-numeric_dates] = as.Date(dates[-numeric_dates], '%m-%d')

# replace date values
colnames(DSHS_deaths_time)[2:ncol(DSHS_deaths_time)] = dates

#save
write.csv(DSHS_deaths_time, 'original-sources/DSHS_county_deaths.csv', row.names = FALSE)

# melt
DSHS_deaths_long = reshape::melt(DSHS_deaths_time, id = c('County'))
colnames(DSHS_deaths_long) = c('County', 'Date', 'Deaths_Cumulative')

# fix most county names (ex: HARRIS -> Harris)
DSHS_deaths_long$County = stringr::str_to_title(DSHS_deaths_long$County)

# manually fix exceptions using DSHS_cases_long as dictionary
mismatched_counties = setdiff(unique(DSHS_deaths_long$County), unique(DSHS_cases_long$County))
DSHS_deaths_long$County = gsub('De Witt', 'DeWitt', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Dewitt', 'DeWitt', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mcculloch', 'McCulloch', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mclennan', 'McLennan', DSHS_deaths_long$County)
DSHS_deaths_long$County = gsub('Mcmullen', 'McMullen', DSHS_deaths_long$County)

# check again - 0 indicates all fixed
length(setdiff(unique(DSHS_deaths_long$County), unique(DSHS_cases_long$County)))

# calculate daily deaths & fill cumulative deaths
DSHS_deaths_long$Deaths_Cumulative = as.integer(as.character(DSHS_deaths_long$Deaths_Cumulative))

DSHS_deaths_long = 
  DSHS_deaths_long %>%
  mutate(Date = as.Date(Date)) %>%
  group_by(County) %>%
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Deaths_Cumulative_NA = Deaths_Cumulative) %>%
  tidyr::fill(Deaths_Cumulative, .direction = "down") %>%
  mutate(Deaths_Daily = c(Deaths_Cumulative[1], diff(Deaths_Cumulative)))
```

## testing

```{r}
# LEGACY
legacy_tests_long = read.csv('original-sources/DSHS_county_tests_legacy_cleaned.csv') %>% 
  mutate(Date = as.Date(Date))

# NEW
test_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19CumulativeTestsbyCounty.xlsx'
temp = tempfile()
download.file(test_url, temp, mode = 'wb')
new_tests = data.frame(readxl::read_excel(temp, sheet = 1))
new_tests_long = new_tests %>% 
  setNames(new_tests[1, ]) %>% 
  filter(str_detect(County, 'County|Unknown|Total', negate = TRUE)) %>% 
  reshape::melt(id = 'County') %>% 
  setNames(c('County', 'Date', 'Tests_Cumulative')) %>% 
  mutate(Date = as.Date(as.integer(Date), '2020-09-12'))

# MERGE
merged_tests = rbind(legacy_tests_long, new_tests_long)

DSHS_tests_long = merged_tests %>%
  group_by(County) %>% 
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Tests_Cumulative_NA = Tests_Cumulative) %>%
  tidyr::fill(Tests_Cumulative, .direction = "down") %>%
  mutate(Tests_Daily = c(Tests_Cumulative[1], diff(Tests_Cumulative)))
```

## new cases

```{r}
# obtained via DSHS county trends dashboard
new_cases_archive = read.csv('original-sources/historical/new-cases/new_case_archive.csv')

# get new cases since 9/1 as excel file
new_case_url ='https://dshs.texas.gov/coronavirus/TexasCOVID-19NewCasesOverTimebyCounty.xlsx'
temp = tempfile()
download.file(new_case_url, temp, mode = 'wb') 

DSHS_new_cases_long = data.frame(readxl::read_excel(temp, sheet = 1, skip = 2)) %>%
  setNames(gsub('New.Cases.', '', colnames(.))) %>% 
  setNames(c('County', format(as.Date(colnames(.)[2:ncol(.)], '%Y.%m.%d'), '%Y-%m-%d'))) %>% 
  reshape2::melt('County') %>% 
  setNames(c('County', 'Date', 'Cases_Daily')) %>% 
  mutate(Date = as.Date(Date)) %>% 
  mutate(Cases_Daily = ifelse(Cases_Daily < 0, 0, Cases_Daily)) %>%
  rbind(., new_cases_archive) %>% 
  arrange(Date, County)
```

## active cases

```{r}
active_case_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19ActiveCaseDatabyCounty.xlsx'
temp = tempfile()
download.file(active_case_url, temp, mode = 'wb') 
DSHS_active_cases_time = data.frame(readxl::read_excel(temp, sheet = 1, skip = 2)) %>%
  slice(which(County == 'Anderson'):which(County == 'Zavala')) %>% 
  dplyr::select(-Notes)

#save
write.csv(DSHS_active_cases_time, 'original-sources/DSHS_county_active_cases.csv', row.names = FALSE)

# melt
DSHS_active_cases_long = reshape::melt(DSHS_active_cases_time, id = 'County') %>% 
  setNames(c('County', 'Date', 'Active_Cases_Cumulative')) %>%
  group_by(Date) %>%
  mutate(Date = as.Date(paste(str_extract_all(Date, '\\d+')[[1]], collapse = ' '), '%m%d')) %>% 
  ungroup() %>%
  group_by(County) %>% 
  unique() %>%
  mutate(Active_Cases_Cumulative = as.integer(as.character(Active_Cases_Cumulative))) %>%
  tidyr::complete(Date = seq.Date(min(Date), date_out, by="day")) %>%
  mutate(Active_Cases_Cumulative_NA = Active_Cases_Cumulative) %>%
  tidyr::fill(Active_Cases_Cumulative, .direction = "down") %>%
  mutate(Active_Cases_Daily = c(Active_Cases_Cumulative[1], diff(Active_Cases_Cumulative))) %>%
  filter(!is.na(County))
```

## childcare

```{r}
# runtime bottleneck: switch pull to once per week (all data is stored in one file)
Get_Childcare_Data = function(date_out) { 
  if (date_out %in% seq(as.Date('2020-10-18'), length.out = 52, by = 'week')) { 
    temp = tempfile()
    download.file('https://apps.hhs.texas.gov/documents/CCR/texas-child-care-facility-based-covid-status.xls',
              temp, mode = 'wb') 
    county_childcare = read_excel_allsheets(temp)
    county_childcare_df = rbindlist(lapply(county_childcare, Clean_Childcare))
    write.csv(county_childcare_df, 'original-sources/county_childcare.csv', row.names = FALSE)
  } else { 
    county_childcare_df = read.csv('original-sources/county_childcare.csv') %>% 
      mutate(Date = as.Date(Date))
    }
  return(county_childcare_df)
}    


Clean_Childcare = function(df, data_type) { 
  date = as.Date(gsub('Data current as of ', '', colnames(df)[1]), '%m/%d/%y')
  colnames(df) = df[3, ]
  
  df = df[4:(nrow(df)-1), c(1, 7:10)]
  
  grouped_df = df %>% 
    filter(County != 'Licensed Child Care Center' & County != 'School-Age Program') %>%
    mutate(County = str_to_title(County)) %>% 
    mutate(County =  gsub('Deafsmith', 'Deaf Smith', County)) %>% 
    mutate(County =  gsub('Dewitt', 'DeWitt', County)) %>% 
    mutate(County =  gsub('Mcculloch', 'McCulloch', County)) %>% 
    mutate(County =  gsub('Mclennan', 'McLennan', County)) %>% 
    group_by(County) %>%
    dplyr::select(2:5) %>%
    mutate_all(as.numeric) %>% 
    summarise_all(sum) %>% 
    setNames(c('County', 'Childcare_Child_Cases_Daily', 'Childcare_Employee_Cases_Daily',
               'Childcare_Child_Cases_Cumulative', 'Childcare_Employee_Cases_Cumulative')) %>% 
    mutate(Date = date)
  return(grouped_df)
}

county_childcare_df = Get_Childcare_Data(date_out)
```

## computed

```{r}
county_school_counts = DSHS_school_df %>% 
  group_by(Date, County) %>% 
  summarize_at(vars(8:ncol(.)-2), funs(sum), na.rm = TRUE)
```


### merge

```{r}
# combine DSHS sources using merge helper function
# https://www.musgraveanalytics.com/blog/2018/2/12/how-to-merge-multiple-data-frames-using-base-r

county_counts = Reduce(function(x, y) merge(x, y, by = c('Date', 'County'), all=TRUE),
       list(DSHS_cases_long, DSHS_new_cases_long, DSHS_deaths_long, DSHS_tests_long,
            DSHS_active_cases_long, county_school_counts, county_childcare_df))
```

## Classifications

```{r}
# add metro and PHR code designations
# source: https://www.dshs.state.tx.us/chs/info/TxCoPhrMsa.xls
county_classifications = read_xlsx('original-sources/county_classifications.xlsx', sheet = 1)[1:254, c(1, 5, 8, 13:14)]

# set colnames
colnames(county_classifications) = c('County', 'PHR', 'Metro_Area', 'HHSC', 'HHSC_Name')

# add PHR readable names from https://dshs.texas.gov/regions/default.shtm
PHR_helper = data.frame(PHR = unique(county_classifications$PHR),
                        PHR_Name = c('Tyler PHR', 'El Paso PHR', 'Harlingen PHR', 
                                     'Arlington PHR', 'Lubbock PHR', 'San Antonio PHR',
                                     'Houston PHR', 'Temple PHR'))

county_classifications = merge(county_classifications, PHR_helper, by = 'PHR')
```

```{r}
# TSA levels
tsa_url = 'https://dshs.texas.gov/coronavirus/TexasCOVID-19HospitalizationsOverTimebyTSA.xlsx'
download.file(tsa_url, 'original-sources/DSHS_tsa_hosp.xlsx', mode = 'wb') 

DSHS_tsa_names = readxl::read_xlsx('original-sources/DSHS_tsa_hosp.xlsx')[3:24, 1:2]
colnames(DSHS_tsa_names) = c('TSA', 'TSA_Name')

# drop . suffix from TSA codes
DSHS_tsa_names$TSA = gsub('.', '', DSHS_tsa_names$TSA, fixed = TRUE)

# get list of counties per TSA
tsa = read.csv('original-sources/tsa_list.csv', header = F)[-1]
tsa_long = reshape::melt(tsa, id = c('V2', 'V3'))
tsa_long_complete = subset(tsa_long, value != '')[, c(1, 4)]
colnames(tsa_long_complete) = c('TSA', 'County')

tsa_long_complete$County = trimws(tsa_long_complete$County)
tsa_long_complete = merge(tsa_long_complete, DSHS_tsa_names, by = 'TSA') %>% unique()
```

## merge

```{r}
# DSHS pop - removed by DSHS 07/30 - defaulting to previous file
dshs_pops = unique(read.csv('https://raw.githubusercontent.com/jeffbrennan/COVID-19/d03d476f7fb060dfd2e1a600a6a1e449df0ab8df/original-sources/DSHS_county_cases.csv')[, c('County', 'Population')])
colnames(dshs_pops) = c('County', 'Population_DSHS')

merged_dshs = Reduce(function(x, y) merge(x, y, by = 'County', all = TRUE),
                       list(county_counts, tsa_long_complete, dshs_pops, county_classifications))

# add TSA, PHR & HHSC combination
merged_dshs$TSA_Combined = paste0(merged_dshs$TSA, ' - ', merged_dshs$TSA_Name)
merged_dshs$PHR_Combined = paste0(merged_dshs$PHR, ' - ', merged_dshs$PHR_Name)
merged_dshs$HHSC_Combined = paste0(merged_dshs$HHSC, ' - ', merged_dshs$HHSC_Name)

merged_county = as.data.frame(merge(merged_dshs, mobility_texas,
                                    by = c('Date', 'County'), all = TRUE)) %>% 
  filter(!is.na(County) & County != 'Unknown')

# fix types
merged_county$County = as.factor(merged_county$County)
merged_county$Population_DSHS = as.numeric(merged_county$Population_DSHS)

# keep only relevant dates (previous dates include google mobility only)
merged_county = merged_county %>% 
  filter(Date >= as.Date('2020-03-04') & !is.na(County)) %>% 
  unique()
```

```{r}
# data dump investigation - move to diagnostics 
daily_tests = merged_county %>% 
  dplyr::select(Date, County, Tests_Cumulative, Tests_Daily) %>% 
  filter(County == 'Harris' & Date > as.Date('2020-09-01'))

ggplot(daily_tests, aes(x = Date, y = Tests_Daily)) + 
  geom_point() +
  geom_line(color = 'red') +
  labs(title = 'Daily Tests since 9/1 in Harris County') + 
  theme_pubr()


# tpr calculation
merged_county %>%
  dplyr::select(Date, County, Cases_Cumulative, Cases_Daily, Tests_Daily) %>%
  group_by(County) %>% 
  mutate(Cases_Cumulative_daily = as.numeric(c(Cases_Cumulative[1], diff(Cases_Cumulative)))) %>%
  filter(County == 'Harris' & Date > Sys.Date() - 14) %>% 
  summarize(TPR_new_cases = sum(Cases_Daily) / sum(Tests_Daily),
            TPR_cumulative_cases = sum(Cases_Cumulative_daily) / sum(Tests_Daily))

```


## TPR 

```{r}
Get_TPR = function() { 
  page = read_html('https://data.cms.gov/stories/s/q5r5-gjyu')
  tpr_url = page %>% html_nodes('a') %>% html_attr('href') %>%
    .[grepl('download', .)] %>% .[1] %>% gsub('%2F', '/', .)
  
  # temp dir needed for unzip to keep directory clean
  temp_dir = tempdir()
  temp_file = tempfile()
  download.file(tpr_url, temp_file, mode = 'wb')
  tpr_df = read_excel(unzip(temp_file, exdir = temp_dir)[[1]])
  tpr_date = as.Date(str_match(tpr_df[2,2], '-(.*)')[2], '%B %d')
  print(tpr_date)
  
  tpr_out = tpr_df %>% 
    setNames(.[which(tpr_df[, 1] == 'County'), ]) %>% 
    filter(State == 'TX') %>% 
    mutate(County = gsub(' County, TX', '', County)) %>% 
    dplyr::select(1, 7, 9) %>% 
    mutate_at(c(2,3), as.numeric) %>% 
    setNames(c('County', 'Tests', 'TPR_CMS')) %>% 
    mutate(Date = tpr_date) %>% 
    dplyr::select(County, Date, Tests, TPR_CMS)

  write.csv(tpr_out, paste0('original-sources/historical/TPR/TPR_', tpr_date, '.csv'),
            row.names = FALSE) 
  }
  
# TODO: find out which day the new file is added
TPR_dates = seq(as.Date('2020-08-19'), by = 'week', length.out = 52)
if (date_out %in% (TPR_dates + 5)) {Get_TPR()}

tpr_df = rbindlist(
         lapply(list.files('original-sources/historical/TPR', full.names = TRUE), read.csv),
         fill = TRUE) %>% 
         mutate(Date = as.Date(Date))

tpr_cases = merged_county %>% 
  dplyr::select(County, Date, Cases_Daily, Population_DSHS) %>%
  filter(Date >= as.Date(min(TPR_dates)) - 13 & Date <= max(tpr_df$Date)) %>%
  group_by(County) %>%
  mutate(Cases_100K_7Day_MA = (rollmean(Cases_Daily, k = 7, align = 'right',
                                        na.pad = TRUE, na.rm = TRUE)
                               / Population_DSHS) * 100000) %>%
  mutate(Cases_100K_14Day_MA = (rollmean(Cases_Daily, k = 14, align = 'right',
                                         na.rm = TRUE, na.pad = TRUE)
                                / Population_DSHS) * 100000) %>% 
  filter(Date %in% TPR_dates) %>% 
  dplyr::select(-Cases_Daily, -Population_DSHS)

tpr_out = tpr_df %>% inner_join(tpr_cases, by = c('County', 'Date')) %>% arrange(County, Date)
write.csv(tpr_out, 'tableau/county_TPR.csv', row.names = FALSE)
```



# TSA LEVEL

## Computed

```{r}
# longitudinal counts (sum)
DSHS_tsa_counts =
    merged_county %>%
    group_by(Date, TSA, TSA_Name) %>% 
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

# static pop counts (sum)
DSHS_tsa_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(TSA) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

# longitudinal google data (mean)
DSHS_tsa_google = 
  merged_county %>%
  group_by(Date, TSA, TSA_Name) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

DSHS_tsa = merge(DSHS_tsa_counts, DSHS_tsa_google, by = c('Date', 'TSA', 'TSA_Name'))
DSHS_tsa = merge(DSHS_tsa, DSHS_tsa_pops, by = 'TSA', all = TRUE)
```

## DSHS hospitals

```{r}
hosp_url = 'https://dshs.texas.gov/coronavirus/CombinedHospitalDataoverTimebyTSA.xlsx'
download.file(hosp_url, 'original-sources/DSHS_tsa_hosp.xlsx', mode = 'wb') 
DSHS_tsa_hosp = read_excel_allsheets('original-sources/DSHS_tsa_hosp.xlsx')
 
DSHS_hosp_clean = function(df, var_name) { 
  colnames(df) = df[1, ]
  df = df[2:23, c(1, 3:ncol(df))]
  df$`TSA ID` = gsub('.', '', df$`TSA ID`, fixed = TRUE)

  # add temp fix for DSHS duplicated dates (8/8) (fixed by DSHS 8/10)
  # TODO: add dynamic handling
  if (length(grep('.x', colnames(df)) > 0)) { 
    df = df[, -grep('.x', colnames(df))]
    colnames(df) = gsub('.y', '', colnames(df))
  }

  # TODO: make date conversion function
  dates = colnames(df)[2:length(colnames(df))]
  numeric_dates = which(is.na(as.Date(dates, format = '%Y-%m-%d')))
  
  # convert from 5 digit excel numeric format
  # https://stackoverflow.com/questions/43230470/how-to-convert-excel-date-format-to-proper-date-in-r
  dates[numeric_dates] = format(as.Date(as.integer(dates[numeric_dates]), origin = '1899-12-30'))
  dates[-numeric_dates] = format(as.Date(dates[-numeric_dates]), '%Y-%m-%d')
  colnames(df)[2:length(colnames(df))] = dates

  df_long = reshape::melt(df, id = 'TSA ID')
  colnames(df_long) = c('TSA', 'Date', var_name)
  df_long$Date = as.Date(df_long$Date)
  
  if(length(which(df_long$Date == '2008-08-08')) > 0) {
    df_long$Date[which(df_long$Date == '2008-08-08')] = as.Date('2020-08-08')
  }
  
  return(df_long)
}
# 1,2,3 references the 3 sheets produced by DSHS
hosp_1 = DSHS_hosp_clean(DSHS_tsa_hosp[[4]], 'Hospitalizations_Total')
hosp_2 = DSHS_hosp_clean(DSHS_tsa_hosp[[6]], 'Hospitalizations_General')
hosp_3 = DSHS_hosp_clean(DSHS_tsa_hosp[[7]], 'Hospitalizations_ICU')
hosp_cap1 = DSHS_hosp_clean(DSHS_tsa_hosp[[8]], 'Beds_Available_Total')
hosp_cap2 = DSHS_hosp_clean(DSHS_tsa_hosp[[9]], 'Beds_Available_ICU')
hosp_cap3 = DSHS_hosp_clean(DSHS_tsa_hosp[[10]], 'Beds_Occupied_Total')
hosp_cap4 = DSHS_hosp_clean(DSHS_tsa_hosp[[11]], 'Beds_Occupied_ICU')
```

## DSHS dashboard

```{r}
# TODO: check on this
# pull DSHS dashboard data using link found from inspect element -> network
DSHS_json_hosp_tsa = jsonlite::fromJSON("https://services5.arcgis.com/ACaLB9ifngzawspq/arcgis/rest/services/DSHS_COVID_Hospital_Data/FeatureServer/0/query?f=json&where=1%3D1&returnGeometry=false&spatialRel=esriSpatialRelIntersects&outFields=*&outSR=102100&resultOffset=0&resultRecordCount=25&resultType=standard&cacheHint=true")[['features']][['attributes']]

DSHS_json_hosp_tsa = DSHS_json_hosp_tsa[, c(2,5:9)]

colnames(DSHS_json_hosp_tsa) = c('TSA', 'Hospital_Beds_Staffed', 'Hospital_Beds_Available',
                                 'ICU_Beds_Available', 'Ventilators_Available', 'Current_Cases')
# DSHS_json_hosp_tsa$Date = format(date_out, '%m/%d/%y')

# add computed cols
DSHS_json_hosp_tsa = DSHS_json_hosp_tsa %>%
                    mutate(Hospital_Beds_Taken = Hospital_Beds_Staffed - Hospital_Beds_Available - Current_Cases)

# export todays file
write.csv(DSHS_json_hosp_tsa, paste0('original-sources/historical/hosp/tsa_hosp_', date_out, '.csv'),
          row.names = FALSE)

# read in all files
hosp_list <- paste0('original-sources/historical/hosp/',
                    list.files(path = 'original-sources/historical/hosp',
                               pattern = '*.csv'))

tsa_all_hosp = lapply(hosp_list, read.csv, fileEncoding = 'UTF-8-BOM')
tsa_combined_hosp = rbindlist(tsa_all_hosp, fill = TRUE)

tsa_combined_hosp$Date = as.Date(tsa_combined_hosp$Date, format = '%m/%d/%y')
# save
write.csv(tsa_combined_hosp, 'original-sources/DSHS_tsa_hosp_detail.csv', row.names = FALSE)


# only keep vents - rest of data is now being posted by DSHS directly 
tsa_combined_hosp = tsa_combined_hosp[, c('TSA', 'Date', 'Ventilators_Available')]
```

## merge

```{r}
merged_tsa = Reduce(function(x, y) merge(x, y, by = c('Date', 'TSA'), all = TRUE),
                    list(DSHS_tsa, tsa_combined_hosp,
                         hosp_1, hosp_2, hosp_3,
                         hosp_cap1, hosp_cap2, hosp_cap3, hosp_cap4))

# fix types & ensure TSA values are valid
merged_tsa = merged_tsa %>% 
  mutate_at(vars(Beds_Available_Total, Beds_Available_ICU, Ventilators_Available,
                 Beds_Occupied_Total, Beds_Occupied_ICU, Hospitalizations_Total,
                 Hospitalizations_General, Hospitalizations_ICU),
            funs(as.character)) %>%
  mutate_at(vars(Beds_Available_Total, Beds_Available_ICU, Ventilators_Available,
                 Beds_Occupied_Total, Beds_Occupied_ICU, Hospitalizations_Total,
                 Hospitalizations_General, Hospitalizations_ICU),
            funs(as.integer)) %>%
  mutate(TSA_Combined = paste0(TSA, ' - ', TSA_Name)) %>%
  filter(!is.na(TSA) & !is.na(Date)) %>% 
  unique()
```

# PHR LEVEL

## computed

```{r}
DSHS_phr_counts =
    merged_county %>%
    group_by(Date, PHR, PHR_Name) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

DSHS_phr_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(PHR) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

DSHS_phr_google = 
  merged_county %>%
  group_by(Date, PHR) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

DSHS_phr = merge(DSHS_phr_counts, DSHS_phr_google, by = c('Date', 'PHR'))
DSHS_phr = merge(DSHS_phr, DSHS_phr_pops, by = c('PHR'), all = TRUE)
```

## alf

(Assisted Living Facilities)

```{r}
# data ending on 7/28
DSHS_alf = read_excel_allsheets('original-sources/historical/phr/DSHS_alf_old.xlsx')

Clean_Facility = function(df, count_type) { 
  df = df[1:8, ]
  colnames(df)[1] = 'PHR'
  df_long = reshape::melt(df, id = 'PHR')
  
  colnames(df_long) = c('PHR', 'Date', count_type)
  df_long$Date = as.Date(as.integer(df_long$Date), origin = '2020-05-14')
  df_long[, 3] = as.numeric(df_long[, 3])
  return(df_long)
  }

alf_fac_totals = Clean_Facility(DSHS_alf[[1]], 'ALF_Total')
alf_cases = Clean_Facility(DSHS_alf[[2]], 'ALF_Cases')
alf_deaths = Clean_Facility(DSHS_alf[[3]], 'ALF_Deaths')
alf_recoveries = Clean_Facility(DSHS_alf[[4]], 'ALF_Recoveries')

alf_df = Reduce(function(x, y) merge(x, y, by = c('Date', 'PHR'), all = TRUE),
                list(alf_fac_totals, alf_cases, alf_deaths, alf_recoveries))
```

## nursing

```{r}
# data ending on 7/28
DSHS_nursing = read_excel_allsheets('original-sources/historical/phr/DSHS_nursing_old.xlsx')

nursing_fac_totals = Clean_Facility(DSHS_nursing[[1]], 'Nursing_Total')
nursing_cases = Clean_Facility(DSHS_nursing[[2]], 'Nursing_Cases')
nursing_deaths = Clean_Facility(DSHS_nursing[[3]], 'Nursing_Deaths')
nursing_recoveries = Clean_Facility(DSHS_nursing[[4]], 'Nursing_Recoveries')

nursing_df = Reduce(function(x, y) merge(x, y, by = c('Date', 'PHR'), all = TRUE),
                list(nursing_fac_totals, nursing_cases, nursing_deaths, nursing_recoveries))
```

## merge

```{r}
phr_df = Reduce(function(x, y) merge(x, y, by = c('Date', 'PHR'), all = TRUE),
                list(DSHS_phr, alf_df, nursing_df))

# address error when knitting despite str(phr_df indicating all calc columns are numeric)
phr_df$Nursing_Cases = as.numeric(phr_df$Nursing_Cases)
phr_df$Nursing_Recoveries = as.numeric(phr_df$Nursing_Recoveries)

phr_df = phr_df %>% filter(!is.na(PHR)) %>% unique()
```

# HHSC Regulation Level

7 levels used for assisted living facility and nursing data (since 7/28)

## computed

```{r}
DSHS_metro_counts =
    merged_county %>%
    group_by(Date, HHSC) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

DSHS_metro_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(HHSC) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

DSHS_metro_google = 
  merged_county %>%
  group_by(Date, HHSC) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)
```

## alf

```{r}
temp = tempfile()
download.file('https://apps.hhs.texas.gov/providers/directories/Texas_Assisted_Living_Facilities_COVID_Summary.xls',
              temp, mode = 'wb') 
hhsc_alf = read_excel_allsheets(temp)

Clean_HHSC = function(df, data_type) { 
  date = as.Date(gsub('Data current as of ', '', colnames(df)[1]), '%B %d, %Y')
  
  first_row = which(df[, 1] == 1)
  last_row =  which(df[, 1] == 7)
  df = df[first_row:last_row, ]
  
  if (ncol(df) == 8) {df = df %>% mutate(Discharges_Cumulative = NA)} 
  else {df = df %>% dplyr::select(1:7, 9, 8)}
  
  colnames(df) = c('HHSC', 'Fac_Outbreak_Active' , 'Fac_Outbreak_Recovered', 'Fac_Outbreak_Cumulative',
                   'Cases_Active', 'Recovered_Cumulative', 'Deaths_Cumulative', 'Cases_Cumulative', 'Discharges_Cumulative')
    
  # add data prefix (nursing or ALF (assisted living facility))
  # start at 2 to ignore HHSC
  colnames(df)[2:ncol(df)] =  paste0(data_type, '_', colnames(df)[2:ncol(df)])
  
  df[['Date']] = date
  
  return(df)
}

hhsc_alf_df = rbindlist(lapply(hhsc_alf, Clean_HHSC, data_type = 'ALF'))
```

## nursing

```{r}
temp = tempfile()
download.file('https://apps.hhs.texas.gov/providers/directories/Texas_Nursing_Facilities_COVID_Summary.xls',
              temp, mode = 'wb') 

hhsc_nursing = read_excel_allsheets(temp)

hhsc_nursing_df = rbindlist(lapply(hhsc_nursing, Clean_HHSC, data_type = 'NURSING'))
```

## merge

```{r}
hhsc_df = merge(hhsc_alf_df, hhsc_nursing_df, by = c('Date', 'HHSC'))
```

# METRO LEVEL

## computed

```{r}
DSHS_metro_counts =
    merged_county %>%
    group_by(Date, Metro_Area) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum))

DSHS_metro_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(Metro_Area) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

DSHS_metro_google = 
  merged_county %>%
  group_by(Date, Metro_Area) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)
```

## merge

```{r}
DSHS_metro = subset(merge(DSHS_metro_counts, DSHS_metro_pops,
                          by = 'Metro_Area', all = TRUE), !is.na(Date))

DSHS_metro = merge(DSHS_metro, DSHS_metro_google, by = c('Date', 'Metro_Area'))
DSHS_metro = DSHS_metro %>% filter(!is.na(Metro_Area)) %>% unique()
```

# STATE LEVEL

## HHS facility reporting

```{r}
# read in files
hhs_fac_reporting_raw = read.csv(url('https://opendata.arcgis.com/datasets/adf5753521ed4e2199d6a4c246e08f84_0.csv'))

hhs_date = as.Date(hhs_fac_reporting_raw$last_updated)
hhs_fac_reporting_raw$Date = hhs_date
hhs_fac_reporting_raw$last_updated = NULL

# combine csv for archival
write_xlsx(list('1' = hhs_fac_reporting_raw),
           path = paste0('original-sources/historical/hhs/hhs_data_', hhs_date, '.xlsx'))

# read in archival sources
hhs_list <- paste0('original-sources/historical/hhs/',
                    list.files(path = 'original-sources/historical/hhs',
                               pattern = '*.xlsx'))

HHS_fac_clean = function(df) { 
    df$Date = format(as.Date(df$Date, origin = '1899-12-30'), '%Y-%m-%d')
    
    if (df$Date[1] <= as.Date('2020-08-03')) {
    
    clean_df = df %>% 
      filter(state_name == 'Texas') %>% 
      dplyr::select(-c('ï..OBJECTID', 'Shape__Area', 'Shape__Length', 'state_fips',
                'state_abbr', 'hhs_region', 'state_name'))
    
    colnames(clean_df)[1:3] = c('reporting_hospitals', 'total_hospitals', 'percent_reporting')
    colnames(clean_df)[1:3] = paste0('HHS_', colnames(clean_df)[1:3])
    
    # drop date from colnames (will likely change over time)
    colnames(clean_df) = gsub('_*\\d', '', colnames(clean_df))
    
  } else { 
    clean_df = df %>% 
      filter(state_name == 'Texas') %>% 
      dplyr::select(-c('ï..OBJECTID', 'state_name'))
    
    colnames(clean_df)[1:3] = paste0('HHS_', colnames(clean_df)[1:3])
    }
  return(clean_df)
}

# 1st lapply: read excel sheets
# 2nd: clean all of them
# rbind cleaned sheets together
HHS_fac_reporting <- rbindlist(lapply(lapply(hhs_list, read_xlsx, sheet = 1), HHS_fac_clean))
```

## HHS hospitalization

```{r}
HHS_hosp_clean = function(df, data_type) { 
  clean_df = df %>% 
    filter(state == 'TX') %>% 
    dplyr::select(-c('state'))
  
  # append type of data to colnames
  colnames(clean_df)[c(3:4, 6:7)] = paste0(data_type, colnames(clean_df[c(3:4, 6:7)]))
  colnames(clean_df) = paste0('HHS_', colnames(clean_df))
  
  clean_df$Date = as.Date(clean_df$HHS_collection_date)
  clean_df$HHS_collection_date = NULL

  return(clean_df)
}
# timeseries files 
HHS_all_inpatient = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/reported_inpatient_all_20200720_0537.csv')),
    'All.Inpatient.')

HHS_covid_inpatient = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/inpatient_covid_final_20200720_0537.csv')),
    'COVID.Inpatient.')

HHS_all_icu = 
  HHS_hosp_clean(
    read.csv(url('https://healthdata.gov/sites/default/files/icu_final_20200720_0537.csv')),
    'All.ICU.')
```

### merge

```{r}
# merge together for combination with other state data
hhs_df = Reduce(function(x, y) merge(x, y, by = 'Date', all = TRUE),
                list(HHS_all_inpatient, HHS_covid_inpatient, HHS_all_icu, HHS_fac_reporting))

```

## DSHS (time series)

```{r}
state_url = 'https://www.dshs.state.tx.us/coronavirus/TexasCOVID19CaseCountData.xlsx'

download.file(state_url, destfile = 'original-sources/DSHS_state.xlsx', mode = 'wb')
dshs_header = colnames(read_excel('original-sources/DSHS_state.xlsx', sheet = 6))[1]

current_year =  substr(Sys.Date(), 1, 4)
dshs_date = paste0(current_year, '/',  str_extract_all(dshs_header, '\\d*\\/\\d*')[[1]])
dshs_date = format(as.Date(dshs_date), '%Y_%m_%d')

# save state level file to historical database for longitudinal demo
download.file(state_url, paste0('original-sources/historical/demo/dshs_',
                                dshs_date, '.xlsx'), mode = 'wb')

DSHS_state = read_excel_allsheets('original-sources/DSHS_state.xlsx')
```

```{r}
DSHS_tests = DSHS_state[[4]]

# # LAB TESTS
# select lab tests & remove rows where all rows are NA
DSHS_lab_tests = DSHS_tests[2:nrow(DSHS_tests), 1:4] %>%
  setNames(c('Date', 'LAB_Tests_Daily', 'LAB_Positive_Tests_Daily','LAB_Positivity_Rate')) %>%
  filter(rowSums(is.na(.)) < ncol(.)) %>%
  mutate(Date = as.Date(as.integer(Date), origin = '1899-12-30'))
#
# # SPECIMEN TESTS
DSHS_specimen_tests = DSHS_tests[2:nrow(DSHS_tests), c(6,7,9,11)] %>%
  setNames(c('Date', 'SPECIMEN_Tests_Daily','SPECIMEN_Positive_Tests_Daily',
             'SPECIMEN_Positivity_Rate')) %>%
  filter(rowSums(is.na(.)) < ncol(.)) %>%
  mutate(Date = as.Date(as.integer(Date), origin = '1899-12-30'))
```

```{r}
# selects covid hospitalization time series, drops footnote, formates date and sets colnames
DSHS_hospitalizations = DSHS_state[[8]][, c(2:3)] %>%
  filter(rowSums(is.na(.)) < ncol(.)) %>% 
  setNames(c('Date', 'Hospitalizations_Total')) %>%
  mutate(Date = as.Date(Date))
```

```{r}
# merge tests & hospitalizations
DSHS_state_time = Reduce(function(x, y) merge(x, y, by = c('Date')),
list(DSHS_specimen_tests, DSHS_lab_tests, DSHS_hospitalizations))

# DSHS_state_time = DSHS_hospitalizations
```

## DSHS (day counts)

```{r}
# avoids duplication of results between sheets
DSHS_state_day = data.frame(Recovered_Total = DSHS_state[[3]][1,1],
                            Active_Total = DSHS_state[[3]][1,2],
                            Antigen_Tests_Total = DSHS_state[[6]][1,2],
                            Antigen_Tests_Positive = DSHS_state[[6]][2,2],
                            Antibody_Tests_Total = DSHS_state[[5]][1,2],
                            Antibody_tests_Positive = DSHS_state[[5]][2,2],
                            Hospital_Bed_Total = DSHS_state[[7]][2,2],
                            Hospital_Bed_Available = DSHS_state[[7]][3,2],
                            ICU_Bed_Available = DSHS_state[[7]][4,2],
                            Ventilator_Available = DSHS_state[[7]][5,2])
```

## DSHS Demographics

```{r}
# TODO: refactor (clean once and append to the cleaned files) - primary script bottleneck
# read in all files
demo.list <- paste0('original-sources/historical/demo/',
                    list.files(path = 'original-sources/historical/demo',
                               pattern = '*.xlsx'))

DSHS_case_age <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 9),
                   lapply(demo.list[67:103], read_excel, sheet = 10),
                   # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 8))
                   lapply(demo.list[113], read_excel, sheet = 8))

DSHS_case_gender <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 10),
                      lapply(demo.list[67:103], read_excel, sheet = 11),
                      # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 9))
                      lapply(demo.list[113], read_excel, sheet = 9))


DSHS_case_race <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 11),
                    lapply(demo.list[67:103], read_excel, sheet = 12),
                    # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 10))
                    lapply(demo.list[113], read_excel, sheet = 10))


DSHS_death_age <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 12),
                    lapply(demo.list[67:103], read_excel, sheet = 13),
                    # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 11))
                    lapply(demo.list[113], read_excel, sheet = 11))


DSHS_death_gender <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 13),
                       lapply(demo.list[67:103], read_excel, sheet = 14),
                       # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 12))
                       lapply(demo.list[113], read_excel, sheet = 12))


DSHS_death_race <- c(lapply(demo.list[c(1:66, 104:112)], read_excel, sheet = 14),
                          lapply(demo.list[67:103], read_excel, sheet = 15),
                          # lapply(demo.list[113:length(demo.list)], read_excel, sheet = 13))
                          lapply(demo.list[113], read_excel, sheet = 13))


DSHS_demo_clean = function(df, data_type) {
  # search for updated date (digit "/" digit)
  date = str_extract_all(colnames(df)[1], '\\d+\\/\\d+')[[1]]

  df = df[2:(nrow(df) - 4), c(1:2)]
  colnames(df) = c('Group', 'var_Cumulative')

  # remove any rows containing total
  total_check = which(df[, 'Group'] == 'Total')
  if (length(total_check) != 0) {df = df[-which(df[, 'Group'] == 'Total'), ]}

  df = df %>%
    mutate(Date = as.Date(paste0('2020/', date))) %>%
    mutate(Group_Type = data_type[1]) %>%
    mutate(var_Cumulative = as.numeric(var_Cumulative)) %>%
    group_by(Date) %>%
    mutate(var_PCT = var_Cumulative / sum(var_Cumulative)) %>%
    na.omit()

  colnames(df) = gsub('var', data_type[2], colnames(df))
  return(df)
}

DSHS_case_age_df = rbindlist(lapply(DSHS_case_age, DSHS_demo_clean, data_type = c('Age', 'Cases')))
DSHS_case_gender_df = rbindlist(lapply(DSHS_case_gender, DSHS_demo_clean, data_type = c('Gender', 'Cases')))
DSHS_case_race_df = rbindlist(lapply(DSHS_case_race, DSHS_demo_clean,  data_type = c('Race', 'Cases')))

DSHS_death_age_df = rbindlist(lapply(DSHS_death_age, DSHS_demo_clean, data_type = c('Age', 'Deaths')))
DSHS_death_gender_df = rbindlist(lapply(DSHS_death_gender, DSHS_demo_clean, data_type = c('Gender', 'Deaths')))
DSHS_death_race_df = rbindlist(lapply(DSHS_death_race, DSHS_demo_clean, data_type = c('Race', 'Deaths')))


# combine case and death cols
DSHS_age_df = merge(DSHS_case_age_df, DSHS_death_age_df, by = c('Date', 'Group_Type',  'Group'))
DSHS_gender_df = merge(DSHS_case_gender_df, DSHS_death_gender_df, by = c('Date', 'Group_Type', 'Group'))
DSHS_race_df = merge(DSHS_case_race_df, DSHS_death_race_df, by = c('Date', 'Group_Type', 'Group'))

demo_stack = rbind(DSHS_age_df, DSHS_gender_df, DSHS_race_df) %>% arrange(Date)
```

## POST 9/25 DEMOGRAPHIC SCRAPING

```{r}
Clean_Demographics = function(index) {
  group_conversion =  c('Age', 'Gender', 'Race')
  base_url = 'https://services5.arcgis.com/ACaLB9ifngzawspq/arcgis/rest/services/DSHS_COVID19_Cases_Service/FeatureServer/'
  
  case_df = fromJSON(paste0(base_url, index, '/query?f=json&where=1%3D1&outFields=*'))[['features']][['attributes']][, c(2:3)] %>% 
    setNames(c('Group', 'Cases_Cumulative')) %>%
    mutate(Date = date_out) %>% 
    mutate(Group_Type = group_conversion[index]) %>% 
    group_by(Date) %>% 
    mutate(Cases_PCT = Cases_Cumulative / sum(Cases_Cumulative)) %>%
    dplyr::select(Date, Group_Type, Group, Cases_Cumulative, Cases_PCT)

  death_df = fromJSON(paste0(base_url, index + 3, '/query?f=json&where=1%3D1&outFields=*'))[['features']][['attributes']][, c(2:3)] %>% 
    setNames(c('Group', 'Deaths_Cumulative')) %>%
    mutate(Date = date_out) %>% 
    mutate(Group_Type = group_conversion[index]) %>% 
    group_by(Date) %>% 
    mutate(Deaths_PCT = Deaths_Cumulative / sum(Deaths_Cumulative)) %>%
    dplyr::select(Date, Group_Type, Group, Deaths_Cumulative, Deaths_PCT)
  
  clean_df = merge(case_df, death_df, by = c('Date', 'Group_Type', 'Group'))
  return(clean_df)
}

demo_releases = seq(as.Date('2020-09-25'), by = 'week', length = 52)

# only run weekly
if (date_out %in% demo_releases) {
  daily_demo_stack = rbindlist(lapply(seq(1,3), Clean_Demographics))
  write.csv(daily_demo_stack, paste0('original-sources/historical/demo-json/dshs_', date_out, '.csv'), row.names = FALSE)
}

```

### merge

```{r}
daily_demo_stack_all = rbindlist(lapply(list.files('original-sources/historical/demo-json/',
                                                   full.names = TRUE), read.csv)) %>% 
  mutate(Date = as.Date(Date)) %>% 
  distinct(Cases_Cumulative, Cases_PCT, Deaths_Cumulative, Deaths_PCT, .keep_all = TRUE)

demo_stacked_combined = rbind(demo_stack, daily_demo_stack_all) %>% 
  group_by(Group_Type, Group) %>% 
  mutate(Cases_Daily = as.numeric(c(Cases_Cumulative[1], diff(Cases_Cumulative)))) %>%
  mutate(Cases_Daily_NO_NEGATIVE = ifelse(Cases_Daily < 0, 0, Cases_Daily)) %>%
  mutate(Deaths_Daily = as.numeric(c(Deaths_Cumulative[1], diff(Deaths_Cumulative)))) %>%
  mutate(Deaths_Daily_NO_NEGATIVE = ifelse(Deaths_Daily < 0, 0, Deaths_Daily)) %>% 
  dplyr::select(Date, Group_Type, Group, Cases_Cumulative, Cases_PCT, Cases_Daily, Cases_Daily_NO_NEGATIVE, everything())
```

## Computed

```{r}
state_counts =
    merged_county %>%
    group_by(Date) %>%
    summarize_at(vars(Cases_Cumulative, Cases_Daily,
                      Deaths_Cumulative, Deaths_Daily,
                      Tests_Cumulative, Tests_Daily,
                      Active_Cases_Cumulative, Active_Cases_Daily),
                 funs(sum(., na.rm = TRUE)))

state_pops = 
  subset(merged_county, Date == '2020-03-04') %>%
  group_by(Date) %>%
  summarize_at(vars(Population_DSHS),
               funs(sum))

state_google = 
  merged_county %>%
  group_by(Date) %>%
  summarize_at(vars(Retail_Recreation, Grocery_Pharmacy,
                    Parks, Transit,
                    Workplaces, Residential),
               funs(weighted.mean(., Population_DSHS)), na.rm = TRUE)

state_facilities = 
  phr_df %>%
  group_by(Date) %>%
  summarize_at(vars(ALF_Total, ALF_Cases, ALF_Deaths, ALF_Recoveries,
                    Nursing_Total, Nursing_Cases, Nursing_Deaths, Nursing_Recoveries),
               funs(sum))

state_hosp_detail = 
  merged_tsa %>% 
  group_by(Date) %>%
  summarize_at(vars(Hospitalizations_Total, Hospitalizations_General, Hospitalizations_ICU,
                    Beds_Available_Total, Beds_Available_ICU,  Beds_Occupied_Total, Beds_Occupied_ICU,
                    Ventilators_Available),
               funs(sum))
```

## merge

```{r}
merged_state = Reduce(function(x, y) merge(x, y, by = c('Date'), all=TRUE),
       list(state_counts, state_google, DSHS_state_time, state_facilities, state_hosp_detail, hhs_df))

merged_state$Population_DSHS = state_pops$Population_DSHS

merged_state = merged_state %>% 
  filter(Date >= as.Date('2020-03-04')) %>% 
  unique()
```

# OUTPUT

## Schools

```{r}
write.csv(nyt_schools, file = 'combined-datasets/schools.csv', row.names = F)
```

## City

```{r}
# write.csv(merged_city, 'tableau/city_pops.csv', row.names = F)
```

## County

```{r}
merged_county_out = merged_county %>% 
  dplyr::select(-c(Cases_Cumulative_NA, Deaths_Cumulative_NA,
                   Tests_Cumulative_NA, Active_Cases_Cumulative_NA))

write.csv(merged_county_out, file = 'combined-datasets/county.csv', row.names = F)
write.csv(merged_county_out, 'tableau/county.csv', row.names = FALSE)
write.csv(county_demo_2018, file = 'combined-datasets/county_demo.csv', row.names = F)
```

## TSA

```{r}
write.csv(merged_tsa, file = 'combined-datasets/tsa.csv', row.names = F)

hosp_tsa = merged_tsa %>% 
  dplyr::select(Date, TSA, TSA_Name, TSA_Combined, Population_DSHS, Ventilators_Available,
                Hospitalizations_Total, Hospitalizations_General, Hospitalizations_ICU,
                Beds_Available_Total, Beds_Available_ICU, Beds_Occupied_Total, Beds_Occupied_ICU) %>% 
  filter(Date >= min(hosp_cap1$Date))
write.csv(hosp_tsa, file = 'tableau/hospitalizations_tsa.csv', row.names = F)
```

## PHR

```{r}
write.csv(phr_df, file = 'combined-datasets/phr.csv', row.names = F)
```

## HHSC

```{r}
write.csv(hhsc_df, file = 'combined-datasets/hhsc.csv', row.names = F)
```

## Metro

```{r}
write.csv(DSHS_metro, file = 'combined-datasets/metro.csv', row.names = F)
```

## State

```{r}
state_out = list("longitudinal" = merged_state, "current" = DSHS_state_day)
write_xlsx(state_out, path = "combined-datasets/state.xlsx")
```

## Demographics

```{r}
write.csv(demo_stacked_combined, file = 'tableau/stacked_demographics.csv', row.names = FALSE)
```

# Performance review

```{r, fig.width=12, fig.height=6}
time_flat = unlist(all_times)
names(time_flat) = NULL

time_df = data.frame('time_sec' = time_flat) %>%
  mutate(chunk = as.numeric(rownames(.)) + 2) %>%
  mutate(version = refactor_version) %>%
  mutate(script = 'covid-scraping.rmd') %>%
  arrange(desc(time_sec))

ggplot(time_df, aes(y = time_sec, x = chunk, fill = time_sec)) + 
  geom_bar(stat = 'identity') + 
  geom_text(aes(label = chunk), position=position_dodge(width = 0.9), vjust = -0.25) + 
  labs(x = 'chunk #', y = 'runtime (seconds)') + 
  scale_fill_gradient(low = 'gray80', high = 'tomato1') + 
  theme_pubr() + 
  theme(axis.text.x = element_blank(),
        legend.position = 'none')
```

## total run time

```{r}
time_df %>% summarize(sum(time_sec))
```


```{r}
write.csv(time_df, paste0('diagnostics/scrape_runtime_', refactor_version, '.csv'),
          row.names = FALSE)
```


